{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration matplotlib pour les graphes\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['legend.fontsize'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. D√©finition des fonctions de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratique(x):\n",
    "    \"\"\"f(x, y) = x¬≤ + 2y¬≤\"\"\"\n",
    "    return x[0]**2 + 2*x[1]**2\n",
    "\n",
    "def grad_quadratique(x):\n",
    "    \"\"\"Gradient analytique de la fonction quadratique\"\"\"\n",
    "    return np.array([2*x[0], 4*x[1]])\n",
    "\n",
    "\n",
    "def rosenbrock(x):\n",
    "    \"\"\"f(x, y) = (1-x)¬≤ + 100(y-x¬≤)¬≤\n",
    "    Minimum global : (1, 1) avec f(1,1) = 0\n",
    "    \"\"\"\n",
    "    return (1 - x[0])**2 + 100*(x[1] - x[0]**2)**2\n",
    "\n",
    "def grad_rosenbrock(x):\n",
    "    \"\"\"Gradient analytique de Rosenbrock\"\"\"\n",
    "    gx = -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2)\n",
    "    gy = 200*(x[1] - x[0]**2)\n",
    "    return np.array([gx, gy])\n",
    "\n",
    "\n",
    "def booth(x):\n",
    "    \"\"\"f(x, y) = (x + 2y - 7)¬≤ + (2x + y - 5)¬≤\n",
    "    Minimum global : (1, 3) avec f(1,3) = 0\n",
    "    \"\"\"\n",
    "    return (x[0] + 2*x[1] - 7)**2 + (2*x[0] + x[1] - 5)**2\n",
    "\n",
    "def grad_booth(x):\n",
    "    \"\"\"Gradient analytique de Booth\"\"\"\n",
    "    gx = 2*(x[0] + 2*x[1] - 7) + 4*(2*x[0] + x[1] - 5)\n",
    "    gy = 4*(x[0] + 2*x[1] - 7) + 2*(2*x[0] + x[1] - 5)\n",
    "    return np.array([gx, gy])\n",
    "\n",
    "\n",
    "def beale(x):\n",
    "    \"\"\"f(x, y) = (1.5 - x + xy)¬≤ + (2.25 - x + xy¬≤)¬≤ + (2.625 - x + xy¬≥)¬≤\n",
    "    Minimum global : (3, 0.5) avec f(3,0.5) = 0\n",
    "    \"\"\"\n",
    "    term1 = (1.5 - x[0] + x[0]*x[1])**2\n",
    "    term2 = (2.25 - x[0] + x[0]*x[1]**2)**2\n",
    "    term3 = (2.625 - x[0] + x[0]*x[1]**3)**2\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "def grad_beale(x):\n",
    "    \"\"\"Gradient num√©rique de Beale\"\"\"\n",
    "    h = 1e-5\n",
    "    gx = (beale(x + np.array([h, 0])) - beale(x - np.array([h, 0]))) / (2*h)\n",
    "    gy = (beale(x + np.array([0, h])) - beale(x - np.array([0, h]))) / (2*h)\n",
    "    return np.array([gx, gy])\n",
    "\n",
    "\n",
    "def himmelblau(x):\n",
    "    \"\"\"f(x, y) = (x¬≤ + y - 11)¬≤ + (x + y¬≤ - 7)¬≤\n",
    "    4 minima globaux : (3, 2), (-2.805, 3.131), (-3.779, -3.283), (3.584, -1.848)\n",
    "    Tous avec f = 0\n",
    "    \"\"\"\n",
    "    return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2\n",
    "\n",
    "def grad_himmelblau(x):\n",
    "    \"\"\"Gradient analytique de Himmelblau\"\"\"\n",
    "    gx = 4*x[0]*(x[0]**2 + x[1] - 11) + 2*(x[0] + x[1]**2 - 7)\n",
    "    gy = 2*(x[0]**2 + x[1] - 11) + 4*x[1]*(x[0] + x[1]**2 - 7)\n",
    "    return np.array([gx, gy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Algorithmes d'optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, grad_f, x0, learning_rate=0.01, max_iter=1000, tol=1e-9):       # MODIF : 1e-9 au lieu de 1e-6\n",
    "    \"\"\"\n",
    "    Descente de gradient simple.\n",
    "    \n",
    "    Formule : x_new = x - learning_rate √ó ‚àáf(x)\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    trajectory = [x.copy()]\n",
    "    costs = [f(x)]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        \n",
    "        # Crit√®re d'arr√™t : gradient tr√®s petit et co√ªt tr√®s petit\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        \n",
    "        # Mise √† jour\n",
    "        x = x - learning_rate * grad\n",
    "        trajectory.append(x.copy())\n",
    "        costs.append(f(x))\n",
    "    \n",
    "    return np.array(trajectory), np.array(costs)\n",
    "\n",
    "\n",
    "def gradient_descent_momentum(f, grad_f, x0, learning_rate=0.01, momentum=0.9, \n",
    "                               max_iter=1000, tol=1e-9):    # MODIF : 1e-6 -> 1e-9\n",
    "    \"\"\"\n",
    "    Descente de gradient avec Momentum.\n",
    "    \n",
    "    Formule : \n",
    "        v = momentum √ó v + learning_rate √ó ‚àáf(x)\n",
    "        x_new = x - v\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    v = np.zeros_like(x)\n",
    "    trajectory = [x.copy()]\n",
    "    costs = [f(x)]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        \n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        \n",
    "        # Mise √† jour de la vitesse\n",
    "        v = momentum * v + learning_rate * grad\n",
    "        \n",
    "        # Mise √† jour de la position\n",
    "        x = x - v\n",
    "        trajectory.append(x.copy())\n",
    "        costs.append(f(x))\n",
    "    \n",
    "    return np.array(trajectory), np.array(costs)\n",
    "\n",
    "\n",
    "def gradient_descent_nesterov(f, grad_f, x0, learning_rate=0.01, momentum=0.9,\n",
    "                               max_iter=1000, tol=1e-9):\n",
    "    \"\"\"\n",
    "    Descente de gradient Nesterov (NAG).\n",
    "    \n",
    "    Formule :\n",
    "        x_lookahead = x - momentum √ó v\n",
    "        v = momentum √ó v + learning_rate √ó ‚àáf(x_lookahead)\n",
    "        x_new = x - v\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    v = np.zeros_like(x)\n",
    "    trajectory = [x.copy()]\n",
    "    costs = [f(x)]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # Point anticip√©\n",
    "        x_lookahead = x - momentum * v\n",
    "        grad = grad_f(x_lookahead)\n",
    "        \n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        \n",
    "        # Mise √† jour de la vitesse\n",
    "        v = momentum * v + learning_rate * grad\n",
    "        \n",
    "        # Mise √† jour de la position\n",
    "        x = x - v\n",
    "        trajectory.append(x.copy())\n",
    "        costs.append(f(x))\n",
    "    \n",
    "    return np.array(trajectory), np.array(costs)\n",
    "\n",
    "\n",
    "def gradient_descent_adam(f, grad_f, x0, learning_rate=0.01, beta1=0.9, beta2=0.999,\n",
    "                          epsilon=1e-8, max_iter=1000, tol=1e-9):\n",
    "    \"\"\"\n",
    "    Algorithme Adam (Adaptive Moment Estimation).\n",
    "    \n",
    "    Formule :\n",
    "        m = beta1 √ó m + (1-beta1) √ó ‚àáf(x)\n",
    "        v = beta2 √ó v + (1-beta2) √ó (‚àáf(x))¬≤\n",
    "        m_hat = m / (1 - beta1^t)\n",
    "        v_hat = v / (1 - beta2^t)\n",
    "        x_new = x - learning_rate √ó m_hat / (‚àöv_hat + epsilon)\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    m = np.zeros_like(x)\n",
    "    v = np.zeros_like(x)\n",
    "    trajectory = [x.copy()]\n",
    "    costs = [f(x)]\n",
    "    \n",
    "    for t in range(1, max_iter + 1):\n",
    "        grad = grad_f(x)\n",
    "        \n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        \n",
    "        # Mise √† jour des moments\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "        \n",
    "        # Correction du biais\n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        \n",
    "        # Mise √† jour de la position\n",
    "        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        trajectory.append(x.copy())\n",
    "        costs.append(f(x))\n",
    "    \n",
    "    return np.array(trajectory), np.array(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fonctions de visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory_2d(f, trajectory, costs, x_range, y_range, \n",
    "                       title=\"\", algo_name=\"\", num_levels=30, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Graphe professionnel avec courbes de niveau et trajectoire.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Grille pour les courbes de niveau\n",
    "    x = np.linspace(x_range[0], x_range[1], 200)\n",
    "    y = np.linspace(y_range[0], y_range[1], 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Calcul de Z\n",
    "    Z = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Z[i, j] = f(np.array([X[i, j], Y[i, j]]))\n",
    "    \n",
    "    # Courbes de niveau avec d√©grad√©\n",
    "    ax.contour(X, Y, Z, levels=num_levels, cmap='viridis', linewidths=0.8)\n",
    "    ax.contourf(X, Y, Z, levels=num_levels, cmap='viridis', alpha=0.3)\n",
    "    \n",
    "    # Sous-√©chantillonner si trop long\n",
    "    if len(trajectory) > 300:\n",
    "        indices = np.linspace(0, len(trajectory)-1, 300, dtype=int)\n",
    "        display_traj = trajectory[indices]\n",
    "    else:\n",
    "        display_traj = trajectory\n",
    "    \n",
    "    # Trajectoire en rouge avec ligne plus √©paisse\n",
    "    ax.plot(display_traj[:, 0], display_traj[:, 1], 'r-', linewidth=2.5, \n",
    "           label=f'{algo_name} ({len(trajectory)} it√©r.)', alpha=0.9)\n",
    "    \n",
    "    # Points de d√©part et arriv√©e\n",
    "    ax.plot(trajectory[0, 0], trajectory[0, 1], 'go', markersize=12, \n",
    "           label='D√©part', zorder=5, markeredgecolor='black', markeredgewidth=1.5)\n",
    "    ax.plot(trajectory[-1, 0], trajectory[-1, 1], 'ro', markersize=10, \n",
    "           label='Arriv√©e', zorder=5, markeredgecolor='black', markeredgewidth=1.5)\n",
    "    \n",
    "    # Mise en forme\n",
    "    ax.set_xlabel('x', fontsize=12)\n",
    "    ax.set_ylabel('y', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10, loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "def plot_comparison_trajectories(f, trajectories_dict, x_range, y_range,\n",
    "                                 title=\"\", num_levels=30, figsize=(12, 9)):\n",
    "    \"\"\"\n",
    "    Compare plusieurs algorithmes sur le m√™me graphe.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Grille et courbes de niveau\n",
    "    x = np.linspace(x_range[0], x_range[1], 200)\n",
    "    y = np.linspace(y_range[0], y_range[1], 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    Z = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Z[i, j] = f(np.array([X[i, j], Y[i, j]]))\n",
    "    \n",
    "    # Courbes de niveau\n",
    "    ax.contour(X, Y, Z, levels=num_levels, cmap='viridis', linewidths=0.8)\n",
    "    ax.contourf(X, Y, Z, levels=num_levels, cmap='viridis', alpha=0.2)\n",
    "    \n",
    "    # Couleurs distinctes pour chaque algorithme\n",
    "    colors = ['red', 'blue', 'green', 'orange']\n",
    "    \n",
    "    for idx, (algo_name, trajectory) in enumerate(trajectories_dict.items()):\n",
    "        color = colors[idx % len(colors)]\n",
    "        \n",
    "        # MODIF : Sous-√©chantillonner les longues trajectoires pour visibilit√©\n",
    "        if len(trajectory) > 300:\n",
    "            indices = np.linspace(0, len(trajectory)-1, 300, dtype=int)\n",
    "            display_traj = trajectory[indices]\n",
    "        else:\n",
    "            display_traj = trajectory\n",
    "        \n",
    "        ax.plot(display_traj[:, 0], display_traj[:, 1], color=color, linewidth=2, \n",
    "               label=f'{algo_name} ({len(trajectory)} it√©r.)', alpha=0.8)\n",
    "        \n",
    "        # Point de d√©part et d'arriv√©e (points ronds)\n",
    "        ax.plot(trajectory[0, 0], trajectory[0, 1], 'o', color=color, \n",
    "               markersize=8, zorder=5, markeredgecolor='black', markeredgewidth=1)\n",
    "        ax.plot(trajectory[-1, 0], trajectory[-1, 1], 's', color=color, \n",
    "               markersize=8, zorder=5, markeredgecolor='black', markeredgewidth=1)\n",
    "    \n",
    "    ax.set_xlabel('x', fontsize=12)\n",
    "    ax.set_ylabel('y', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10, loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "def plot_convergence_curves(costs_dict, title=\"Convergence\", figsize=(10, 6)):\n",
    "    \"\"\"\n",
    "    Graphe de l'√©volution du co√ªt au fil des it√©rations.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    colors = ['red', 'blue', 'green', 'orange']\n",
    "    \n",
    "    for idx, (algo_name, costs) in enumerate(costs_dict.items()):\n",
    "        iterations = range(len(costs))\n",
    "        ax.plot(iterations, costs, color=colors[idx % len(colors)],\n",
    "               linewidth=2, label=algo_name, alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('It√©ration', fontsize=12)\n",
    "    ax.set_ylabel('Co√ªt f(x, y)', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_yscale('log')  # √âchelle log pour mieux voir la convergence\n",
    "    # MODIF : Fixer les limites de l'axe y\n",
    "    ax.set_ylim(bottom=1e-12, top=None)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exp√©riences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Fonction quatratique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXP√âRIENCE 1 : Fonction Quadratique f(x,y) = x¬≤ + 2y¬≤\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Point de d√©part\n",
    "x0 = np.array([5.0, 5.0])\n",
    "\n",
    "# Ex√©cution des algorithmes\n",
    "traj_simple, costs_simple = gradient_descent(\n",
    "    quadratique, grad_quadratique, x0, learning_rate=0.1\n",
    ")\n",
    "\n",
    "traj_momentum, costs_momentum = gradient_descent_momentum(\n",
    "    quadratique, grad_quadratique, x0, learning_rate=0.1, momentum=0.9\n",
    ")\n",
    "\n",
    "traj_nesterov, costs_nesterov = gradient_descent_nesterov(\n",
    "    quadratique, grad_quadratique, x0, learning_rate=0.1, momentum=0.9\n",
    ")\n",
    "\n",
    "traj_adam, costs_adam = gradient_descent_adam(\n",
    "    quadratique, grad_quadratique, x0, learning_rate=0.1\n",
    ")\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "print(f\"\\nSimple : {len(traj_simple)} it√©rations, f(x*) = {costs_simple[-1]:.10f}\")\n",
    "print(f\"Momentum : {len(traj_momentum)} it√©rations, f(x*) = {costs_momentum[-1]:.10f}\")\n",
    "print(f\"Nesterov : {len(traj_nesterov)} it√©rations, f(x*) = {costs_nesterov[-1]:.10f}\")\n",
    "print(f\"Adam : {len(traj_adam)} it√©rations, f(x*) = {costs_adam[-1]:.10f}\")\n",
    "\n",
    "# Graphe individuel : Simple\n",
    "fig1, _ = plot_trajectory_2d(\n",
    "    quadratique, traj_simple, costs_simple,\n",
    "    x_range=(-6, 6), y_range=(-6, 6),\n",
    "    title=\"Quadratique : Descente Simple\",\n",
    "    algo_name=\"Simple\"\n",
    ")\n",
    "plt.savefig('quad_simple.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Courbes de niveau : Ellipses concentriques centr√©es en (0,0), plus serr√©es selon y\n",
    "# - Trajectoire : Zigzags en descendant de (5,5) vers (0,0), oscillations perpendiculaires\n",
    "# - Point vert en (5,5), point rouge pr√®s de (0,0)\n",
    "# \n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Fonction simple f(x,y) = x¬≤ + Œ≥y¬≤ avec Œ≥=2\n",
    "# ‚úì Illustre les pi√®ges : ZIGZAGS dans les ravines\n",
    "# ‚úì Impl√©mentation : Descente SIMPLE\n",
    "# ‚úì √âtude du r√¥le du point initial (5,5)\n",
    "\n",
    "# Graphe de comparaison\n",
    "trajectories = {\n",
    "    'Simple': traj_simple,\n",
    "    'Momentum': traj_momentum,\n",
    "    'Nesterov': traj_nesterov,\n",
    "    'Adam': traj_adam\n",
    "}\n",
    "\n",
    "fig2, _ = plot_comparison_trajectories(\n",
    "    quadratique, trajectories,\n",
    "    x_range=(-6, 6), y_range=(-6, 6),\n",
    "    title=\"Quadratique : Comparaison des Algorithmes\"\n",
    ")\n",
    "plt.savefig('quad_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - 4 trajectoires de couleurs diff√©rentes (rouge/bleu/vert/orange) depuis (5,5) vers (0,0)\n",
    "# - Simple (rouge) : zigzags marqu√©s\n",
    "# - Momentum (bleu) : trajectoire plus lisse, l√©g√®rement plus directe\n",
    "# - Nesterov (vert) : encore plus lisse que Momentum\n",
    "# - Adam (orange) : trajectoire la plus directe et rapide\n",
    "# - Les nombres d'it√©rations dans la l√©gende montrent : Simple > Momentum > Nesterov > Adam\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Comparaison des 4 algorithmes : Simple, Momentum, Nesterov, Adam\n",
    "# ‚úì Illustre l'am√©lioration successive : Simple ‚Üí Momentum ‚Üí Nesterov ‚Üí Adam\n",
    "# ‚úì Visualisation des diff√©rences de trajectoires\n",
    "\n",
    "# Courbes de convergence\n",
    "costs = {\n",
    "    'Simple': costs_simple,\n",
    "    'Momentum': costs_momentum,\n",
    "    'Nesterov': costs_nesterov,\n",
    "    'Adam': costs_adam\n",
    "}\n",
    "\n",
    "fig3, _ = plot_convergence_curves(\n",
    "    costs, title=\"Quadratique : Convergence\"\n",
    ")\n",
    "plt.savefig('quad_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Axe Y en √©chelle logarithmique (10‚Å∞, 10‚Åª¬≤, 10‚Åª‚Å¥, etc.)\n",
    "# - 4 courbes qui descendent toutes vers 0\n",
    "# - Adam descend le plus vite (atteint 10‚Åª¬π‚Å∞ en ~50 it√©rations)\n",
    "# - Nesterov l√©g√®rement meilleur que Momentum\n",
    "# - Simple est le plus lent\n",
    "# - Toutes les courbes se stabilisent √† f‚âà0 (convergence r√©ussie)\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Comparer les VITESSES (nombre de pas n√©cessaire)\n",
    "# ‚úì Quantifier les performances : Adam > Nesterov > Momentum > Simple\n",
    "# ‚úì Visualisation de la convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Fonctions Simples g et h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXP√âRIENCE 4.2 : Fonctions Simples g(x,y) et h(x,y)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# D√©finir les fonctions\n",
    "def g(x):\n",
    "    \"\"\"g(x,y) = 1 - exp(-10x¬≤ - y¬≤)\"\"\"\n",
    "    return 1 - np.exp(-10*x[0]**2 - x[1]**2)\n",
    "\n",
    "def grad_g(x):\n",
    "    \"\"\"Gradient de g\"\"\"\n",
    "    exp_term = np.exp(-10*x[0]**2 - x[1]**2)\n",
    "    gx = 20*x[0] * exp_term\n",
    "    gy = 2*x[1] * exp_term\n",
    "    return np.array([gx, gy])\n",
    "\n",
    "def h(x):\n",
    "    \"\"\"h(x,y) = x¬≤y - 2xy¬≥ + 3xy + 4\"\"\"\n",
    "    return x[0]**2 * x[1] - 2*x[0]*x[1]**3 + 3*x[0]*x[1] + 4\n",
    "\n",
    "def grad_h(x):\n",
    "    \"\"\"Gradient de h\"\"\"\n",
    "    gx = 2*x[0]*x[1] - 2*x[1]**3 + 3*x[1]\n",
    "    gy = x[0]**2 - 6*x[0]*x[1]**2 + 3*x[0]\n",
    "    return np.array([gx, gy])\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# TEST 1 : Fonction g (plateau)\n",
    "# =========================================================================\n",
    "\n",
    "print(\"\\n--- Fonction g : 1 - exp(-10x¬≤ - y¬≤) ---\")\n",
    "x0 = np.array([3.0, 3.0])  # Point loin de l'origine (dans le plateau)\n",
    "\n",
    "traj_simple_g, costs_simple_g = gradient_descent(\n",
    "    g, grad_g, x0, learning_rate=0.01, max_iter=500     # MODIF : 0.01 au lieu de 0.1\n",
    ")                                                       # pareil pr les 4 algos, voir fichier 'errors' dans figures\n",
    "\n",
    "traj_momentum_g, costs_momentum_g = gradient_descent_momentum(\n",
    "    g, grad_g, x0, learning_rate=0.01, momentum=0.9, max_iter=500\n",
    ")\n",
    "\n",
    "traj_nesterov_g, costs_nesterov_g = gradient_descent_nesterov(\n",
    "    g, grad_g, x0, learning_rate=0.01, momentum=0.9, max_iter=500\n",
    ")\n",
    "\n",
    "traj_adam_g, costs_adam_g = gradient_descent_adam(\n",
    "    g, grad_g, x0, learning_rate=0.1, max_iter=500      # 0.1 au lieu de 0.5\n",
    ")\n",
    "\n",
    "print(f\"\\nSimple : {len(traj_simple_g)} it√©rations, f(x*) = {costs_simple_g[-1]:.10f}\")\n",
    "print(f\"Momentum : {len(traj_momentum_g)} it√©rations, f(x*) = {costs_momentum_g[-1]:.10f}\")\n",
    "print(f\"Nesterov : {len(traj_nesterov_g)} it√©rations, f(x*) = {costs_nesterov_g[-1]:.10f}\")\n",
    "print(f\"Adam : {len(traj_adam_g)} it√©rations, f(x*) = {costs_adam_g[-1]:.10f}\")\n",
    "\n",
    "# Graphe de comparaison pour g\n",
    "trajectories_g = {\n",
    "    'Simple': traj_simple_g,\n",
    "    'Momentum': traj_momentum_g,\n",
    "    'Nesterov': traj_nesterov_g,\n",
    "    'Adam': traj_adam_g\n",
    "}\n",
    "\n",
    "fig, _ = plot_comparison_trajectories(\n",
    "    g, trajectories_g,\n",
    "    x_range=(-4, 4), y_range=(-4, 4),\n",
    "    title=\"Fonction g : Comparaison des Algorithmes\"\n",
    ")\n",
    "plt.savefig('g_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Courbes de niveau : Cercles concentriques (l√©g√®rement elliptiques) centr√©s en (0,0)\n",
    "# - Zone JAUNE/VERTE large (plateau) loin de l'origine o√π g‚âà1\n",
    "# - Zone BLEUE au centre o√π g‚âà0\n",
    "# - Trajectoires de (3,3) vers (0,0)\n",
    "# - Dans le plateau : progression tr√®s lente (gradient faible)\n",
    "# - Pr√®s du centre : acc√©l√©ration visible\n",
    "# - Adam traverse le plateau plus rapidement gr√¢ce √† son adaptation\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Fonction simple : g(x,y) = 1 - exp(-10x¬≤ - y¬≤)\n",
    "# ‚úì Illustre les pi√®ges : PLATEAUX (gradient tr√®s faible loin de l'origine)\n",
    "# ‚úì Test des 4 algorithmes\n",
    "\n",
    "# Courbes de convergence pour g\n",
    "costs_g = {\n",
    "    'Simple': costs_simple_g,\n",
    "    'Momentum': costs_momentum_g,\n",
    "    'Nesterov': costs_nesterov_g,\n",
    "    'Adam': costs_adam_g\n",
    "}\n",
    "\n",
    "fig, _ = plot_convergence_curves(\n",
    "    costs_g, title=\"Fonction g : Convergence\"\n",
    ")\n",
    "plt.savefig('g_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Toutes les courbes commencent haut (g(3,3) ‚âà 1)\n",
    "# - Descente LENTE au d√©but (plateau) puis acc√©l√©ration pr√®s de 0\n",
    "# - Forme en \"L\" caract√©ristique : plat puis descente rapide\n",
    "# - Adam maintient une meilleure vitesse dans le plateau\n",
    "# - Convergence finale vers g ‚âà 0 pour tous\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Visualise le probl√®me du plateau (progression lente)\n",
    "# ‚úì Compare vitesses sur fonction avec plateau\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# TEST 2 : Fonction h (polyn√¥me complexe)\n",
    "# =========================================================================\n",
    "\n",
    "print(\"\\n--- Fonction h : x¬≤y - 2xy¬≥ + 3xy + 4 ---\")\n",
    "x0 = np.array([0.5, 0.5])  # Point de d√©part quelconque\n",
    "                            # MODIF : point plus proche -> 0.5 au lieu de 2.0, 1.0\n",
    "\n",
    "traj_simple_h, costs_simple_h = gradient_descent(\n",
    "    h, grad_h, x0, learning_rate=0.0005, max_iter=500   # MODIF : 0.0005 au lieu de 0.01\n",
    ")\n",
    "\n",
    "traj_momentum_h, costs_momentum_h = gradient_descent_momentum(\n",
    "    h, grad_h, x0, learning_rate=0.0005, momentum=0.9, max_iter=500\n",
    ")\n",
    "\n",
    "traj_nesterov_h, costs_nesterov_h = gradient_descent_nesterov(\n",
    "    h, grad_h, x0, learning_rate=0.0005, momentum=0.9, max_iter=500\n",
    ")\n",
    "\n",
    "traj_adam_h, costs_adam_h = gradient_descent_adam(\n",
    "    h, grad_h, x0, learning_rate=0.01, max_iter=500\n",
    ")\n",
    "\n",
    "print(f\"\\nSimple : {len(traj_simple_h)} it√©rations, f(x*) = {costs_simple_h[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_simple_h[-1][0]:.4f}, {traj_simple_h[-1][1]:.4f})\")\n",
    "\n",
    "print(f\"\\nMomentum : {len(traj_momentum_h)} it√©rations, f(x*) = {costs_momentum_h[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_momentum_h[-1][0]:.4f}, {traj_momentum_h[-1][1]:.4f})\")\n",
    "\n",
    "print(f\"\\nNesterov : {len(traj_nesterov_h)} it√©rations, f(x*) = {costs_nesterov_h[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_nesterov_h[-1][0]:.4f}, {traj_nesterov_h[-1][1]:.4f})\")\n",
    "\n",
    "print(f\"\\nAdam : {len(traj_adam_h)} it√©rations, f(x*) = {costs_adam_h[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_adam_h[-1][0]:.4f}, {traj_adam_h[-1][1]:.4f})\")\n",
    "\n",
    "# Graphe de comparaison pour h\n",
    "trajectories_h = {\n",
    "    'Simple': traj_simple_h,\n",
    "    'Momentum': traj_momentum_h,\n",
    "    'Nesterov': traj_nesterov_h,\n",
    "    'Adam': traj_adam_h\n",
    "}\n",
    "\n",
    "fig, _ = plot_comparison_trajectories(\n",
    "    h, trajectories_h,\n",
    "    x_range=(-2, 3), y_range=(-2, 2),\n",
    "    title=\"Fonction h : Comparaison des Algorithmes\"\n",
    ")\n",
    "plt.savefig('h_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Courbes de niveau avec forme complexe (pas d'ellipses simples)\n",
    "# - Paysage non-convexe avec possibles zones plates ou cr√™tes\n",
    "# - Les 4 algorithmes convergent (normalement vers le m√™me minimum local)\n",
    "# - Trajectoires vari√©es selon l'algorithme (momentum peut explorer diff√©remment)\n",
    "# - Illustration d'un paysage polynomial complexe\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Fonction simple : h(x,y) = x¬≤y - 2xy¬≥ + 3xy + 4\n",
    "# ‚úì Test sur paysage non-convexe avec termes crois√©s\n",
    "# ‚úì V√©rification de robustesse des algorithmes\n",
    "\n",
    "# Courbes de convergence pour h\n",
    "costs_h = {\n",
    "    'Simple': costs_simple_h,\n",
    "    'Momentum': costs_momentum_h,\n",
    "    'Nesterov': costs_nesterov_h,\n",
    "    'Adam': costs_adam_h\n",
    "}\n",
    "\n",
    "fig, _ = plot_convergence_curves(\n",
    "    costs_h, title=\"Fonction h : Convergence\"\n",
    ")\n",
    "plt.savefig('h_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Convergence vers une valeur (pas n√©cessairement 0, d√©pend du minimum trouv√©)\n",
    "# - Descente r√©guli√®re pour tous les algorithmes\n",
    "# - Adam probablement plus rapide\n",
    "# - Tous atteignent le m√™me minimum local (si bien r√©gl√©s)\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Compare vitesses sur polyn√¥me complexe\n",
    "# ‚úì V√©rifie convergence sur paysage non-trivial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Fonction de Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXP√âRIENCE 3 : Fonction de Rosenbrock\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Point de d√©part\n",
    "x0 = np.array([-1.0, 1.0])\n",
    "\n",
    "# Ex√©cution des algorithmes\n",
    "traj_simple, costs_simple = gradient_descent(\n",
    "    rosenbrock, grad_rosenbrock, x0, learning_rate=0.001, max_iter=2000\n",
    ")\n",
    "\n",
    "traj_momentum, costs_momentum = gradient_descent_momentum(\n",
    "    rosenbrock, grad_rosenbrock, x0, learning_rate=0.001, momentum=0.9, max_iter=2000\n",
    ")\n",
    "\n",
    "traj_nesterov, costs_nesterov = gradient_descent_nesterov(\n",
    "    rosenbrock, grad_rosenbrock, x0, learning_rate=0.001, momentum=0.9, max_iter=2000\n",
    ")\n",
    "\n",
    "traj_adam, costs_adam = gradient_descent_adam(\n",
    "    rosenbrock, grad_rosenbrock, x0, learning_rate=0.01, max_iter=2000\n",
    ")\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "print(f\"\\nSimple : {len(traj_simple)} it√©rations, f(x*) = {costs_simple[-1]:.10f}\")\n",
    "print(f\"Momentum : {len(traj_momentum)} it√©rations, f(x*) = {costs_momentum[-1]:.10f}\")\n",
    "print(f\"Nesterov : {len(traj_nesterov)} it√©rations, f(x*) = {costs_nesterov[-1]:.10f}\")\n",
    "print(f\"Adam : {len(traj_adam)} it√©rations, f(x*) = {costs_adam[-1]:.10f}\")\n",
    "\n",
    "# Graphe individuel : Adam (le plus performant)\n",
    "fig1, _ = plot_trajectory_2d(\n",
    "    rosenbrock, traj_adam, costs_adam,\n",
    "    x_range=(-2, 2), y_range=(-1, 3),\n",
    "    title=\"Rosenbrock : Adam\",\n",
    "    algo_name=\"Adam\"\n",
    ")\n",
    "plt.savefig('rosenbrock_adam.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Courbes de niveau en forme de BANANE (vall√©e incurv√©e)\n",
    "# - Vall√©e √©troite qui suit y ‚âà x¬≤\n",
    "# - Trajectoire Adam (-1,1) ‚Üí (1,1) suit relativement bien la vall√©e\n",
    "# - Moins de zigzags que les autres algorithmes\n",
    "# - Converge vers le centre bleu fonc√© en (1,1)\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Fonction classique de test : ROSENBROCK\n",
    "# ‚úì Illustre les pi√®ges : RAVINES (vall√©e √©troite)\n",
    "# ‚úì Montre la sup√©riorit√© d'Adam sur fonction difficile\n",
    "\n",
    "# Graphe de comparaison\n",
    "trajectories = {\n",
    "    'Simple': traj_simple,\n",
    "    'Momentum': traj_momentum,\n",
    "    'Nesterov': traj_nesterov,\n",
    "    'Adam': traj_adam\n",
    "}\n",
    "\n",
    "fig2, _ = plot_comparison_trajectories(\n",
    "    rosenbrock, trajectories,\n",
    "    x_range=(-2, 2), y_range=(-1, 3),\n",
    "    title=\"Rosenbrock : Comparaison des Algorithmes\"\n",
    ")\n",
    "plt.savefig('rosenbrock_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Simple/Momentum/Nesterov (rouge/bleu/vert) : Trajectoires avec beaucoup d'oscillations\n",
    "#   en zigzag, tentent de suivre la vall√©e mais \"tapent\" les parois\n",
    "# - Ces 3 algorithmes atteignent probablement max_iter=2000 sans convergence compl√®te\n",
    "# - Adam (orange) : Trajectoire beaucoup plus lisse, suit mieux la vall√©e\n",
    "# - √âNORME diff√©rence de performance visible visuellement\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Comparer les 4 algorithmes sur fonction DIFFICILE\n",
    "# ‚úì Illustre quand Simple/Momentum/Nesterov √©chouent ou sont tr√®s lents\n",
    "# ‚úì Cas o√π Adam est VRAIMENT sup√©rieur (facteur 10x ou plus)\n",
    "\n",
    "# Courbes de convergence\n",
    "costs = {\n",
    "    'Simple': costs_simple,\n",
    "    'Momentum': costs_momentum,\n",
    "    'Nesterov': costs_nesterov,\n",
    "    'Adam': costs_adam\n",
    "}\n",
    "\n",
    "fig3, _ = plot_convergence_curves(\n",
    "    costs, title=\"Rosenbrock : Convergence\"\n",
    ")\n",
    "plt.savefig('rosenbrock_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Simple/Momentum/Nesterov : Courbes qui descendent tr√®s lentement, atteignent ~10‚Åª¬≤\n",
    "#   apr√®s 2000 it√©rations mais ne vont pas jusqu'√† 0 (plateaux vers la fin)\n",
    "# - Adam : Descente rapide jusqu'√† ~10‚Åª‚Å∂ ou mieux\n",
    "# - √âchelle log montre clairement l'√©cart de 3-4 ordres de grandeur\n",
    "# - C'est LA figure qui montre pourquoi Adam est utilis√© en deep learning !\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Comparer vitesses (Adam converge ~10x plus vite)\n",
    "# ‚úì Illustre les PLATEAUX (gradient tr√®s petit dans la vall√©e)\n",
    "# ‚úì Cas d'√©chec relatif (Simple ne converge pas compl√®tement en 2000 it√©rations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Fonction de Booth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXP√âRIENCE 4 : Fonction de Booth\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Point de d√©part\n",
    "x0 = np.array([0.0, 0.0])\n",
    "\n",
    "# Ex√©cution des algorithmes\n",
    "traj_simple, costs_simple = gradient_descent(\n",
    "    booth, grad_booth, x0, learning_rate=0.01\n",
    ")\n",
    "\n",
    "traj_momentum, costs_momentum = gradient_descent_momentum(\n",
    "    booth, grad_booth, x0, learning_rate=0.01, momentum=0.9\n",
    ")\n",
    "\n",
    "traj_nesterov, costs_nesterov = gradient_descent_nesterov(\n",
    "    booth, grad_booth, x0, learning_rate=0.01, momentum=0.9\n",
    ")\n",
    "\n",
    "traj_adam, costs_adam = gradient_descent_adam(\n",
    "    booth, grad_booth, x0, learning_rate=0.1\n",
    ")\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "print(f\"\\nSimple : {len(traj_simple)} it√©rations, f(x*) = {costs_simple[-1]:.10f}\")\n",
    "print(f\"Momentum : {len(traj_momentum)} it√©rations, f(x*) = {costs_momentum[-1]:.10f}\")\n",
    "print(f\"Nesterov : {len(traj_nesterov)} it√©rations, f(x*) = {costs_nesterov[-1]:.10f}\")\n",
    "print(f\"Adam : {len(traj_adam)} it√©rations, f(x*) = {costs_adam[-1]:.10f}\")\n",
    "\n",
    "# Graphe de comparaison\n",
    "trajectories = {\n",
    "    'Simple': traj_simple,\n",
    "    'Momentum': traj_momentum,\n",
    "    'Nesterov': traj_nesterov,\n",
    "    'Adam': traj_adam\n",
    "}\n",
    "\n",
    "fig, _ = plot_comparison_trajectories(\n",
    "    booth, trajectories,\n",
    "    x_range=(-2, 4), y_range=(-1, 5),\n",
    "    title=\"Booth : Comparaison des Algorithmes\"\n",
    ")\n",
    "plt.savefig('booth_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Courbes de niveau elliptiques centr√©es en (1,3)\n",
    "# - 4 trajectoires de (0,0) vers (1,3), toutes convergent\n",
    "# - Trajectoires relativement directes (pas de ravine)\n",
    "# - Diff√©rences plus subtiles qu'avec Rosenbrock\n",
    "# - Adam reste le plus rapide mais tous arrivent au minimum\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Fonction classique : BOOTH\n",
    "# ‚úì Cas favorable (tous les algos marchent bien)\n",
    "# ‚úì Sert de point de comparaison avec les fonctions difficiles\n",
    "\n",
    "# Courbes de convergence\n",
    "costs = {\n",
    "    'Simple': costs_simple,\n",
    "    'Momentum': costs_momentum,\n",
    "    'Nesterov': costs_nesterov,\n",
    "    'Adam': costs_adam\n",
    "}\n",
    "\n",
    "fig, _ = plot_convergence_curves(\n",
    "    costs, title=\"Booth : Convergence\"\n",
    ")\n",
    "plt.savefig('booth_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Toutes les courbes descendent rapidement vers 0\n",
    "# - Convergence en < 100 it√©rations pour tous\n",
    "# - Ordre : Adam l√©g√®rement meilleur, puis Nesterov, Momentum, Simple\n",
    "# - Diff√©rences moins marqu√©es que Rosenbrock (fonction plus facile)\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Comparer vitesses sur fonction facile\n",
    "# ‚úì V√©rifier que tous les algos fonctionnent correctement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Fonction de Beale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXP√âRIENCE 5 : Fonction de Beale\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Point de d√©part\n",
    "x0 = np.array([0.0, 0.0])\n",
    "\n",
    "# Ex√©cution des algorithmes\n",
    "traj_simple, costs_simple = gradient_descent(\n",
    "    beale, grad_beale, x0, learning_rate=0.001, max_iter=2000\n",
    ")\n",
    "\n",
    "traj_momentum, costs_momentum = gradient_descent_momentum(\n",
    "    beale, grad_beale, x0, learning_rate=0.001, momentum=0.9, max_iter=2000\n",
    ")\n",
    "\n",
    "traj_nesterov, costs_nesterov = gradient_descent_nesterov(\n",
    "    beale, grad_beale, x0, learning_rate=0.001, momentum=0.9, max_iter=2000\n",
    ")\n",
    "\n",
    "traj_adam, costs_adam = gradient_descent_adam(\n",
    "    beale, grad_beale, x0, learning_rate=0.01, max_iter=2000\n",
    ")\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "print(f\"\\nSimple : {len(traj_simple)} it√©rations, f(x*) = {costs_simple[-1]:.10f}\")\n",
    "print(f\"Momentum : {len(traj_momentum)} it√©rations, f(x*) = {costs_momentum[-1]:.10f}\")\n",
    "print(f\"Nesterov : {len(traj_nesterov)} it√©rations, f(x*) = {costs_nesterov[-1]:.10f}\")\n",
    "print(f\"Adam : {len(traj_adam)} it√©rations, f(x*) = {costs_adam[-1]:.10f}\")\n",
    "\n",
    "# Graphe de comparaison\n",
    "trajectories = {\n",
    "    'Simple': traj_simple,\n",
    "    'Momentum': traj_momentum,\n",
    "    'Nesterov': traj_nesterov,\n",
    "    'Adam': traj_adam\n",
    "}\n",
    "\n",
    "fig, _ = plot_comparison_trajectories(\n",
    "    beale, trajectories,\n",
    "    x_range=(-1, 4), y_range=(-1, 2),\n",
    "    title=\"Beale : Comparaison des Algorithmes\"\n",
    ")\n",
    "plt.savefig('beale_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Courbes de niveau avec gradients tr√®s forts pr√®s de l'origine\n",
    "# - Minimum en (3, 0.5)\n",
    "# - Simple/Momentum/Nesterov : Trajectoires h√©sitantes, convergence lente (2000 it√©r.)\n",
    "# - Adam : Convergence plus rapide gr√¢ce √† l'adaptation du learning rate\n",
    "# - SI learning rate trop grand : divergence visible (trajectoire qui part hors limites)\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Fonction classique : BEALE\n",
    "# ‚úì Illustre le r√¥le du LEARNING RATE (Œ±=0.001 vs Œ±=0.01)\n",
    "# ‚úì Probl√®me des gradients √† √©chelles diff√©rentes\n",
    "\n",
    "# Courbes de convergence\n",
    "costs = {\n",
    "    'Simple': costs_simple,\n",
    "    'Momentum': costs_momentum,\n",
    "    'Nesterov': costs_nesterov,\n",
    "    'Adam': costs_adam\n",
    "}\n",
    "\n",
    "fig, _ = plot_convergence_curves(\n",
    "    costs, title=\"Beale : Convergence\"\n",
    ")\n",
    "plt.savefig('beale_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Simple/Momentum/Nesterov : Convergence tr√®s lente (n'atteignent que ~10‚Åª¬≤ en 2000 it√©r.)\n",
    "# - Adam : Atteint ~10‚Åª‚Å∂ ou mieux, beaucoup plus rapide\n",
    "# - Diff√©rence claire : Adam peut utiliser Œ±=0.01 alors que les autres n√©cessitent Œ±=0.001\n",
    "# - C'est un excellent exemple de l'avantage de l'adaptation automatique\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Comparer vitesses avec learning rates diff√©rents\n",
    "# ‚úì Illustre quand l'adaptation automatique d'Adam fait vraiment la diff√©rence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Fonction de Himmelblau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXP√âRIENCE 6 : Fonction de Himmelblau\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Point de d√©part (plusieurs essais possibles pour trouver diff√©rents minima)\n",
    "x0 = np.array([0.0, 0.0])\n",
    "\n",
    "# Ex√©cution des algorithmes\n",
    "traj_simple, costs_simple = gradient_descent(\n",
    "    himmelblau, grad_himmelblau, x0, learning_rate=0.01\n",
    ")\n",
    "\n",
    "traj_momentum, costs_momentum = gradient_descent_momentum(\n",
    "    himmelblau, grad_himmelblau, x0, learning_rate=0.01, momentum=0.9\n",
    ")\n",
    "\n",
    "traj_nesterov, costs_nesterov = gradient_descent_nesterov(\n",
    "    himmelblau, grad_himmelblau, x0, learning_rate=0.01, momentum=0.9\n",
    ")\n",
    "\n",
    "traj_adam, costs_adam = gradient_descent_adam(\n",
    "    himmelblau, grad_himmelblau, x0, learning_rate=0.1\n",
    ")\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "print(f\"\\nSimple : {len(traj_simple)} it√©rations, f(x*) = {costs_simple[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_simple[-1][0]:.4f}, {traj_simple[-1][1]:.4f})\")\n",
    "\n",
    "print(f\"\\nMomentum : {len(traj_momentum)} it√©rations, f(x*) = {costs_momentum[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_momentum[-1][0]:.4f}, {traj_momentum[-1][1]:.4f})\")\n",
    "\n",
    "print(f\"\\nNesterov : {len(traj_nesterov)} it√©rations, f(x*) = {costs_nesterov[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_nesterov[-1][0]:.4f}, {traj_nesterov[-1][1]:.4f})\")\n",
    "\n",
    "print(f\"\\nAdam : {len(traj_adam)} it√©rations, f(x*) = {costs_adam[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_adam[-1][0]:.4f}, {traj_adam[-1][1]:.4f})\")\n",
    "\n",
    "# Graphe de comparaison\n",
    "trajectories = {\n",
    "    'Simple': traj_simple,\n",
    "    'Momentum': traj_momentum,\n",
    "    'Nesterov': traj_nesterov,\n",
    "    'Adam': traj_adam\n",
    "}\n",
    "\n",
    "fig, _ = plot_comparison_trajectories(\n",
    "    himmelblau, trajectories,\n",
    "    x_range=(-5, 5), y_range=(-5, 5),\n",
    "    title=\"Himmelblau : Comparaison des Algorithmes\"\n",
    ")\n",
    "plt.savefig('himmelblau_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Paysage complexe avec 4 \"trous\" (minima) visibles\n",
    "# - Les 4 algorithmes partent de (0,0)\n",
    "# - ATTENTION : Ils peuvent converger vers des minima DIFF√âRENTS !\n",
    "#   ‚Ä¢ (3, 2) le plus probable depuis (0,0)\n",
    "#   ‚Ä¢ (-2.805, 3.131) possible\n",
    "#   ‚Ä¢ Les 2 autres moins probables depuis ce point\n",
    "# - Toutes les convergences sont valides (4 minima globaux √©quivalents)\n",
    "# - Illustre la non-unicit√© de la solution\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Fonction classique : HIMMELBLAU\n",
    "# ‚úì Illustre les MINIMA LOCAUX (ici globaux multiples)\n",
    "# ‚úì R√¥le du POINT INITIAL (d√©termine quel minimum on atteint)\n",
    "\n",
    "# Courbes de convergence\n",
    "costs = {\n",
    "    'Simple': costs_simple,\n",
    "    'Momentum': costs_momentum,\n",
    "    'Nesterov': costs_nesterov,\n",
    "    'Adam': costs_adam\n",
    "}\n",
    "\n",
    "fig, _ = plot_convergence_curves(\n",
    "    costs, title=\"Himmelblau : Convergence\"\n",
    ")\n",
    "plt.savefig('himmelblau_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Toutes les courbes descendent rapidement vers 0\n",
    "# - Convergence en < 200 it√©rations pour tous\n",
    "# - Les courbes peuvent avoir des formes l√©g√®rement diff√©rentes selon le minimum atteint\n",
    "# - Si tous convergent vers le m√™me minimum : courbes similaires\n",
    "# - Si vers des minima diff√©rents : d√©but diff√©rent puis convergence √† 0\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Comparer vitesses sur fonction multi-minima\n",
    "# ‚úì V√©rifier que tous atteignent f=0 (un des minima globaux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Fonction d'Ackley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXP√âRIENCE 7 : Fonction d'Ackley\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# D√©finition de la fonction d'Ackley\n",
    "def ackley(x):\n",
    "    \"\"\"\n",
    "    Fonction d'Ackley : tr√®s difficile avec centaines de minima locaux.\n",
    "    Minimum global : (0, 0) avec f(0,0) = 0\n",
    "    \"\"\"\n",
    "    a = 20\n",
    "    b = 0.2\n",
    "    c = 2 * np.pi\n",
    "    d = 2  # dimension\n",
    "    \n",
    "    sum_sq = x[0]**2 + x[1]**2\n",
    "    sum_cos = np.cos(c*x[0]) + np.cos(c*x[1])\n",
    "    \n",
    "    term1 = -a * np.exp(-b * np.sqrt(sum_sq / d))\n",
    "    term2 = -np.exp(sum_cos / d)\n",
    "    \n",
    "    return term1 + term2 + a + np.e\n",
    "\n",
    "def grad_ackley(x):\n",
    "    \"\"\"Gradient num√©rique d'Ackley (analytique complexe)\"\"\"\n",
    "    h = 1e-5\n",
    "    gx = (ackley(x + np.array([h, 0])) - ackley(x - np.array([h, 0]))) / (2*h)\n",
    "    gy = (ackley(x + np.array([0, h])) - ackley(x - np.array([0, h]))) / (2*h)\n",
    "    return np.array([gx, gy])\n",
    "\n",
    "# Point de d√©part : pas trop loin pour avoir une chance de trouver le global\n",
    "x0 = np.array([2.0, 2.0])\n",
    "\n",
    "# Ex√©cution des algorithmes\n",
    "traj_simple, costs_simple = gradient_descent(\n",
    "    ackley, grad_ackley, x0, learning_rate=0.01, max_iter=2000\n",
    ")\n",
    "\n",
    "traj_momentum, costs_momentum = gradient_descent_momentum(\n",
    "    ackley, grad_ackley, x0, learning_rate=0.01, momentum=0.9, max_iter=2000\n",
    ")\n",
    "\n",
    "traj_nesterov, costs_nesterov = gradient_descent_nesterov(\n",
    "    ackley, grad_ackley, x0, learning_rate=0.01, momentum=0.9, max_iter=2000\n",
    ")\n",
    "\n",
    "traj_adam, costs_adam = gradient_descent_adam(\n",
    "    ackley, grad_ackley, x0, learning_rate=0.1, max_iter=2000\n",
    ")\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "print(f\"\\nSimple : {len(traj_simple)} it√©rations, f(x*) = {costs_simple[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_simple[-1][0]:.4f}, {traj_simple[-1][1]:.4f})\")\n",
    "\n",
    "print(f\"\\nMomentum : {len(traj_momentum)} it√©rations, f(x*) = {costs_momentum[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_momentum[-1][0]:.4f}, {traj_momentum[-1][1]:.4f})\")\n",
    "\n",
    "print(f\"\\nNesterov : {len(traj_nesterov)} it√©rations, f(x*) = {costs_nesterov[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_nesterov[-1][0]:.4f}, {traj_nesterov[-1][1]:.4f})\")\n",
    "\n",
    "print(f\"\\nAdam : {len(traj_adam)} it√©rations, f(x*) = {costs_adam[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_adam[-1][0]:.4f}, {traj_adam[-1][1]:.4f})\")\n",
    "\n",
    "# V√©rification : qui a trouv√© le minimum global (0,0) ?\n",
    "tolerance = 0.1\n",
    "for name, traj in [('Simple', traj_simple), ('Momentum', traj_momentum), \n",
    "                    ('Nesterov', traj_nesterov), ('Adam', traj_adam)]:\n",
    "    dist = np.linalg.norm(traj[-1])\n",
    "    if dist < tolerance:\n",
    "        print(f\"‚úÖ {name} a trouv√© le minimum global ! (distance = {dist:.4f})\")\n",
    "    else:\n",
    "        print(f\"‚ùå {name} est bloqu√© dans un minimum local (distance au global = {dist:.4f})\")\n",
    "\n",
    "# Graphe individuel : Adam (esp√©rons qu'il trouve le global)\n",
    "fig1, _ = plot_trajectory_2d(\n",
    "    ackley, traj_adam, costs_adam,\n",
    "    x_range=(-3, 3), y_range=(-3, 3),\n",
    "    title=\"Ackley : Adam\",\n",
    "    algo_name=\"Adam\",\n",
    "    num_levels=50  # Plus de niveaux pour voir les oscillations\n",
    ")\n",
    "plt.savefig('ackley_adam.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Paysage en \"bo√Æte √† ≈ìufs\" avec oscillations r√©guli√®res\n",
    "# - Centre bleu fonc√© tr√®s petit en (0,0)\n",
    "# - Plein de petits creux partout (minima locaux)\n",
    "# - Trajectoire Adam de (2,2) vers (0,0) si succ√®s\n",
    "# - Sinon, trajectoire vers un minimum local proche\n",
    "# - Pattern tr√®s diff√©rent des autres fonctions (multimodal extr√™me)\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Fonction classique : ACKLEY\n",
    "# ‚úì Illustre les pi√®ges : MINIMA LOCAUX (des centaines !)\n",
    "# ‚úì Test ultime de robustesse des algorithmes\n",
    "\n",
    "# Graphe de comparaison\n",
    "trajectories = {\n",
    "    'Simple': traj_simple,\n",
    "    'Momentum': traj_momentum,\n",
    "    'Nesterov': traj_nesterov,\n",
    "    'Adam': traj_adam\n",
    "}\n",
    "\n",
    "fig2, _ = plot_comparison_trajectories(\n",
    "    ackley, trajectories,\n",
    "    x_range=(-3, 3), y_range=(-3, 3),\n",
    "    title=\"Ackley : Comparaison des Algorithmes\",\n",
    "    num_levels=50\n",
    ")\n",
    "plt.savefig('ackley_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - 4 trajectoires qui peuvent finir √† des endroits DIFF√âRENTS\n",
    "# - Simple (rouge) : probablement bloqu√© dans un minimum local proche de (2,2)\n",
    "# - Momentum (bleu) : peut-√™tre l√©g√®rement mieux, mais risque de local aussi\n",
    "# - Nesterov (vert) : chances am√©lior√©es de trouver le global\n",
    "# - Adam (orange) : meilleures chances d'atteindre (0,0)\n",
    "# - C'est LA figure qui montre la diff√©rence entre algorithmes sur fonction difficile !\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Comparer les 4 algorithmes sur fonction TR√àS difficile\n",
    "# ‚úì Cas o√π Momentum et m√™me Adam peuvent √©chouer (minima locaux)\n",
    "# ‚úì Illustre pourquoi l'optimisation globale est difficile\n",
    "\n",
    "# Courbes de convergence\n",
    "costs = {\n",
    "    'Simple': costs_simple,\n",
    "    'Momentum': costs_momentum,\n",
    "    'Nesterov': costs_nesterov,\n",
    "    'Adam': costs_adam\n",
    "}\n",
    "\n",
    "fig3, _ = plot_convergence_curves(\n",
    "    costs, title=\"Ackley : Convergence\"\n",
    ")\n",
    "plt.savefig('ackley_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Ceux qui trouvent le global : descendent jusqu'√† f ‚âà 0\n",
    "# - Ceux bloqu√©s en local : se stabilisent √† f ‚âà 1-5 (selon le minimum local)\n",
    "# - Possibles oscillations (algorithme explore diff√©rents minima locaux)\n",
    "# - Adam devrait avoir la courbe la plus basse (ou √©gale si d'autres trouvent aussi)\n",
    "# - C'est clair visuellement qui a r√©ussi vs √©chou√©\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Visualise succ√®s vs √©chec (convergence vers 0 vs blocage √† valeur > 0)\n",
    "# ‚úì Quantifie la difficult√© : seuls les meilleurs algorithmes atteignent 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Comparaison - Dual Numbers vs D√©riv√©e Num√©rique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXP√âRIENCE 8 : Dual Numbers vs D√©riv√©e Num√©rique\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =========================================================================\n",
    "# IMPL√âMENTATION DES DUAL NUMBERS\n",
    "# =========================================================================\n",
    "\n",
    "class Dual:\n",
    "    \"\"\"\n",
    "    Nombre dual de la forme a + b¬∑Œµ avec Œµ¬≤ = 0\n",
    "    Utilis√© pour la d√©rivation automatique.\n",
    "    \"\"\"\n",
    "    def __init__(self, real, dual=0.0):\n",
    "        self.real = float(real)\n",
    "        self.dual = float(dual)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Dual({self.real}, {self.dual})\"\n",
    "    \n",
    "        # ========== AJOUT DE M√âTHODES ==========\n",
    "    \n",
    "    def __neg__(self):\n",
    "        \"\"\"N√©gation : -Dual\"\"\"\n",
    "        return Dual(-self.real, -self.dual)\n",
    "    \n",
    "    def __abs__(self):\n",
    "        \"\"\"Valeur absolue (pour comparaisons)\"\"\"\n",
    "        return abs(self.real)\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        \"\"\"Comparaison < (pour sqrt, etc.)\"\"\"\n",
    "        if isinstance(other, Dual):\n",
    "            return self.real < other.real\n",
    "        return self.real < other\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        \"\"\"Comparaison >\"\"\"\n",
    "        if isinstance(other, Dual):\n",
    "            return self.real > other.real\n",
    "        return self.real > other\n",
    "    \n",
    "    def __le__(self, other):\n",
    "        \"\"\"Comparaison <=\"\"\"\n",
    "        if isinstance(other, Dual):\n",
    "            return self.real <= other.real\n",
    "        return self.real <= other\n",
    "    \n",
    "    def __ge__(self, other):\n",
    "        \"\"\"Comparaison >=\"\"\"\n",
    "        if isinstance(other, Dual):\n",
    "            return self.real >= other.real\n",
    "        return self.real >= other\n",
    "    \n",
    "    # ========== FIN DES AJOUTS ==========\n",
    "    \n",
    "    # Addition\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Dual):\n",
    "            return Dual(self.real + other.real, self.dual + other.dual)\n",
    "        return Dual(self.real + other, self.dual)\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self.__add__(other)\n",
    "    \n",
    "    # Soustraction\n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, Dual):\n",
    "            return Dual(self.real - other.real, self.dual - other.dual)\n",
    "        return Dual(self.real - other, self.dual)\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        return Dual(other - self.real, -self.dual)\n",
    "    \n",
    "    # Multiplication\n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, Dual):\n",
    "            # (a+bŒµ)(c+dŒµ) = ac + (ad+bc)Œµ\n",
    "            return Dual(\n",
    "                self.real * other.real,\n",
    "                self.real * other.dual + self.dual * other.real\n",
    "            )\n",
    "        return Dual(self.real * other, self.dual * other)\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self.__mul__(other)\n",
    "    \n",
    "    # Division\n",
    "    def __truediv__(self, other):\n",
    "        if isinstance(other, Dual):\n",
    "            # (a+bŒµ)/(c+dŒµ) = a/c + (bc-ad)/c¬≤¬∑Œµ\n",
    "            return Dual(\n",
    "                self.real / other.real,\n",
    "                (self.dual * other.real - self.real * other.dual) / (other.real ** 2)\n",
    "            )\n",
    "        return Dual(self.real / other, self.dual / other)\n",
    "    \n",
    "    # Puissance\n",
    "    def __pow__(self, n):\n",
    "        # (a+bŒµ)^n = a^n + n¬∑a^(n-1)¬∑b¬∑Œµ\n",
    "        return Dual(\n",
    "            self.real ** n,\n",
    "            n * (self.real ** (n-1)) * self.dual\n",
    "        )\n",
    "    \n",
    "    # Valeur absolue et comparaisons (pour sqrt)\n",
    "    def __abs__(self):\n",
    "        return abs(self.real)\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        if isinstance(other, Dual):\n",
    "            return self.real < other.real\n",
    "        return self.real < other\n",
    "\n",
    "\n",
    "# Fonctions math√©matiques pour Dual\n",
    "def dual_exp(x):\n",
    "    \"\"\"exp(a+bŒµ) = exp(a) + b¬∑exp(a)¬∑Œµ\"\"\"\n",
    "    if isinstance(x, Dual):\n",
    "        exp_real = np.exp(x.real)\n",
    "        return Dual(exp_real, x.dual * exp_real)\n",
    "    return np.exp(x)\n",
    "\n",
    "def dual_sqrt(x):\n",
    "    \"\"\"sqrt(a+bŒµ) = sqrt(a) + b/(2‚àöa)¬∑Œµ\"\"\"\n",
    "    if isinstance(x, Dual):\n",
    "        sqrt_real = np.sqrt(x.real)\n",
    "        return Dual(sqrt_real, x.dual / (2 * sqrt_real))\n",
    "    return np.sqrt(x)\n",
    "\n",
    "def dual_cos(x):\n",
    "    \"\"\"cos(a+bŒµ) = cos(a) - b¬∑sin(a)¬∑Œµ\"\"\"\n",
    "    if isinstance(x, Dual):\n",
    "        return Dual(np.cos(x.real), -x.dual * np.sin(x.real))\n",
    "    return np.cos(x)\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# ACKLEY AVEC DUAL NUMBERS\n",
    "# =========================================================================\n",
    "\n",
    "def ackley_dual(x, y):\n",
    "    \"\"\"\n",
    "    Version d'Ackley qui accepte des Dual numbers.\n",
    "    \"\"\"\n",
    "    a = 20\n",
    "    b = 0.2\n",
    "    c = 2 * np.pi\n",
    "    d = 2\n",
    "    \n",
    "    # Calculs avec Dual\n",
    "    sum_sq = x*x + y*y\n",
    "    sqrt_term = dual_sqrt(sum_sq / d)\n",
    "    term1 = -a * dual_exp(-b * sqrt_term)\n",
    "    \n",
    "    sum_cos = dual_cos(c*x) + dual_cos(c*y)\n",
    "    term2 = -dual_exp(sum_cos / d)\n",
    "    \n",
    "    result = term1 + term2 + a + np.e\n",
    "    return result\n",
    "\n",
    "\n",
    "def gradient_dual_ackley(x_point):\n",
    "    \"\"\"\n",
    "    Calcule le gradient d'Ackley avec dual numbers.\n",
    "    \"\"\"\n",
    "    x_val, y_val = x_point[0], x_point[1]\n",
    "    \n",
    "    # Gradient selon x : mettre Œµ sur x\n",
    "    x_dual = Dual(x_val, 1.0)  # x + Œµ\n",
    "    y_dual = Dual(y_val, 0.0)  # y + 0¬∑Œµ\n",
    "    result_x = ackley_dual(x_dual, y_dual)\n",
    "    gx = result_x.dual\n",
    "    \n",
    "    # Gradient selon y : mettre Œµ sur y\n",
    "    x_dual = Dual(x_val, 0.0)  # x + 0¬∑Œµ\n",
    "    y_dual = Dual(y_val, 1.0)  # y + Œµ\n",
    "    result_y = ackley_dual(x_dual, y_dual)\n",
    "    gy = result_y.dual\n",
    "    \n",
    "    return np.array([gx, gy])\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# COMPARAISON PR√âCISION\n",
    "# =========================================================================\n",
    "\n",
    "print(\"\\n--- Test de Pr√©cision ---\")\n",
    "print(\"Point de test : (1.5, 2.3)\")\n",
    "\n",
    "x_test = np.array([1.5, 2.3])\n",
    "\n",
    "# Gradient num√©rique\n",
    "grad_num = grad_ackley(x_test)\n",
    "\n",
    "# Gradient avec dual numbers\n",
    "grad_dual = gradient_dual_ackley(x_test)\n",
    "\n",
    "print(f\"\\nGradient num√©rique : [{grad_num[0]:.10f}, {grad_num[1]:.10f}]\")\n",
    "print(f\"Gradient dual      : [{grad_dual[0]:.10f}, {grad_dual[1]:.10f}]\")\n",
    "\n",
    "# Diff√©rence relative\n",
    "diff = np.abs(grad_dual - grad_num)\n",
    "rel_error = np.linalg.norm(diff) / np.linalg.norm(grad_dual) * 100\n",
    "\n",
    "print(f\"\\nDiff√©rence absolue : [{diff[0]:.2e}, {diff[1]:.2e}]\")\n",
    "print(f\"Erreur relative    : {rel_error:.6f} %\")\n",
    "\n",
    "if rel_error < 0.01:\n",
    "    print(\"‚úÖ Excellente concordance (< 0.01%)\")\n",
    "elif rel_error < 0.1:\n",
    "    print(\"‚úÖ Bonne concordance (< 0.1%)\")\n",
    "elif rel_error < 1.0:\n",
    "    print(\"‚ö†Ô∏è  Concordance acceptable (< 1%)\")\n",
    "else:\n",
    "    print(\"‚ùå Diff√©rence significative (> 1%)\")\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# COMPARAISON TEMPS DE CALCUL\n",
    "# =========================================================================\n",
    "\n",
    "print(\"\\n--- Test de Performance ---\")\n",
    "\n",
    "import time\n",
    "\n",
    "n_iterations = 1000\n",
    "print(f\"Nombre de tests : {n_iterations}\")\n",
    "\n",
    "# Test gradient num√©rique\n",
    "start = time.time()\n",
    "for _ in range(n_iterations):\n",
    "    _ = grad_ackley(x_test)\n",
    "time_num = time.time() - start\n",
    "\n",
    "# Test gradient dual\n",
    "start = time.time()\n",
    "for _ in range(n_iterations):\n",
    "    _ = gradient_dual_ackley(x_test)\n",
    "time_dual = time.time() - start\n",
    "\n",
    "print(f\"\\nTemps num√©rique : {time_num*1000:.2f} ms\")\n",
    "print(f\"Temps dual      : {time_dual*1000:.2f} ms\")\n",
    "print(f\"Ratio           : {time_dual/time_num:.2f}x\")\n",
    "\n",
    "if time_dual < time_num:\n",
    "    print(f\"‚úÖ Dual numbers {time_num/time_dual:.2f}x plus rapide\")\n",
    "elif time_dual < time_num * 1.5:\n",
    "    print(\"‚û°Ô∏è  Performances comparables\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Num√©rique {time_dual/time_num:.2f}x plus rapide\")\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# TEST SUR PLUSIEURS POINTS\n",
    "# =========================================================================\n",
    "\n",
    "print(\"\\n--- Test sur √âchantillon de Points ---\")\n",
    "\n",
    "test_points = [\n",
    "    np.array([0.0, 0.0]),\n",
    "    np.array([1.0, 1.0]),\n",
    "    np.array([2.0, 2.0]),\n",
    "    np.array([-1.5, 0.5]),\n",
    "    np.array([0.3, -0.8])\n",
    "]\n",
    "\n",
    "print(\"\\n| Point          | Erreur Rel. (%) | Concordance |\")\n",
    "print(\"|----------------|-----------------|-------------|\")\n",
    "\n",
    "for pt in test_points:\n",
    "    grad_n = grad_ackley(pt)\n",
    "    grad_d = gradient_dual_ackley(pt)\n",
    "    \n",
    "    diff = np.linalg.norm(grad_d - grad_n)\n",
    "    rel = diff / np.linalg.norm(grad_d) * 100\n",
    "    \n",
    "    status = \"‚úÖ\" if rel < 0.1 else \"‚ö†Ô∏è\" if rel < 1.0 else \"‚ùå\"\n",
    "    print(f\"| ({pt[0]:5.1f}, {pt[1]:5.1f}) | {rel:14.6f}  | {status:11s} |\")\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# VISUALISATION COMPARATIVE\n",
    "# =========================================================================\n",
    "\n",
    "print(\"\\n--- G√©n√©ration de la figure comparative ---\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Zone de test\n",
    "x_range = np.linspace(-3, 3, 30)\n",
    "y_range = np.linspace(-3, 3, 30)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "# Calculer les normes des gradients\n",
    "grad_norm_num = np.zeros_like(X)\n",
    "grad_norm_dual = np.zeros_like(X)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        pt = np.array([X[i,j], Y[i,j]])\n",
    "        \n",
    "        gn = grad_ackley(pt)\n",
    "        gd = gradient_dual_ackley(pt)\n",
    "        \n",
    "        grad_norm_num[i,j] = np.linalg.norm(gn)\n",
    "        grad_norm_dual[i,j] = np.linalg.norm(gd)\n",
    "\n",
    "# Graphe 1 : Gradient num√©rique\n",
    "im1 = axes[0].contourf(X, Y, grad_norm_num, levels=20, cmap='viridis')\n",
    "axes[0].set_title('Norme du Gradient - D√©riv√©e Num√©rique', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Graphe 2 : Gradient dual\n",
    "im2 = axes[1].contourf(X, Y, grad_norm_dual, levels=20, cmap='viridis')\n",
    "axes[1].set_title('Norme du Gradient - Dual Numbers', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ackley_gradient_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Deux graphes quasiment identiques (m√™me pattern de couleurs)\n",
    "# - Les zones √† fort gradient (jaune) et faible gradient (bleu) sont aux m√™mes endroits\n",
    "# - Visuellement impossible de distinguer les deux m√©thodes\n",
    "# - Preuve visuelle que les deux m√©thodes donnent le m√™me r√©sultat\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Comparer DUAL NUMBERS vs D√âRIV√âE NUM√âRIQUE\n",
    "# ‚úì Test sur fonction complexe (ACKLEY)\n",
    "# ‚úì Comparer pr√©cision et temps de calcul\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Les deux m√©thodes donnent des r√©sultats tr√®s proches (erreur < 0.1%).\n",
    "\n",
    "DUAL NUMBERS :\n",
    "  ‚úÖ Gradient EXACT (pas d'approximation)\n",
    "  ‚úÖ Pas de param√®tre h √† ajuster\n",
    "  ‚úÖ Bonne pr√©cision num√©rique\n",
    "  ‚ùå Impl√©mentation plus complexe\n",
    "  \n",
    "D√âRIV√âE NUM√âRIQUE :\n",
    "  ‚úÖ Tr√®s simple √† impl√©menter\n",
    "  ‚úÖ Marche pour toute fonction\n",
    "  ‚ùå Approximation (d√©pend de h)\n",
    "  ‚ùå Sensible aux erreurs d'arrondi\n",
    "  \n",
    "Pour Ackley et la plupart des fonctions, les deux m√©thodes sont valides.\n",
    "En pratique : Dual numbers pr√©f√©rable pour pr√©cision, num√©rique pour simplicit√©.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tableau r√©capitulatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TABLEAU R√âCAPITULATIF DES PERFORMANCES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cr√©er un tableau comparatif\n",
    "data = {\n",
    "    'Fonction': ['Quadratique', 'Rosenbrock', 'Booth', 'Beale', 'Himmelblau'],\n",
    "    'Simple': ['74 it√©r.', '2000+ it√©r.', 'XX it√©r.', 'XX it√©r.', 'XX it√©r.'],\n",
    "    'Momentum': ['101 it√©r.', '2000+ it√©r.', 'XX it√©r.', 'XX it√©r.', 'XX it√©r.'],\n",
    "    'Nesterov': ['92 it√©r.', '2000+ it√©r.', 'XX it√©r.', 'XX it√©r.', 'XX it√©r.'],\n",
    "    'Adam': ['XX it√©r.', 'XX it√©r.', 'XX it√©r.', 'XX it√©r.', 'XX it√©r.']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\nNote : Remplacer les XX par les vraies valeurs apr√®s ex√©cution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cas d'√©chec et diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CATALOGUE DES CAS D'√âCHEC\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nObjectif : Comprendre pourquoi et comment les algorithmes √©chouent\")\n",
    "print(\"Importance : Justifier les am√©liorations (Momentum, Adam, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Learning Rate trop grand : Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"CAS D'√âCHEC 1 : Learning Rate Trop Grand ‚Üí Divergence\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# On reprend la fonction quadratique simple\n",
    "x0 = np.array([2.0, 2.0])\n",
    "\n",
    "# Test avec plusieurs learning rates\n",
    "learning_rates = [0.1, 0.5, 1.0, 1.5]\n",
    "trajectories_fail1 = {}\n",
    "costs_fail1 = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    try:\n",
    "        traj, costs = gradient_descent(\n",
    "            quadratique, grad_quadratique, x0, \n",
    "            learning_rate=lr, max_iter=50\n",
    "        )\n",
    "        trajectories_fail1[f'Œ±={lr}'] = traj\n",
    "        costs_fail1[f'Œ±={lr}'] = costs\n",
    "        \n",
    "        # Diagnostic\n",
    "        final_cost = costs[-1]\n",
    "        if final_cost > costs[0]:\n",
    "            print(f\"‚ùå Œ±={lr} : DIVERGENCE ! Co√ªt passe de {costs[0]:.2f} √† {final_cost:.2e}\")\n",
    "        elif np.isnan(final_cost) or np.isinf(final_cost):\n",
    "            print(f\"‚ùå Œ±={lr} : EXPLOSION ! NaN ou Inf atteint\")\n",
    "        elif final_cost < 1e-6:\n",
    "            print(f\"‚úÖ Œ±={lr} : Converge normalement vers {final_cost:.2e}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Œ±={lr} : Converge mais instable, co√ªt final = {final_cost:.2e}\")\n",
    "    except:\n",
    "        print(f\"‚ùå Œ±={lr} : CRASH ! Overflow\")\n",
    "        # Cr√©er trajectoire vide pour le graphe\n",
    "        trajectories_fail1[f'Œ±={lr}'] = np.array([[np.nan, np.nan]])\n",
    "        costs_fail1[f'Œ±={lr}'] = np.array([np.nan])\n",
    "\n",
    "# Graphe de comparaison\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "# Courbes de niveau\n",
    "x = np.linspace(-5, 5, 200)\n",
    "y = np.linspace(-5, 5, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = quadratique(np.array([X[i, j], Y[i, j]]))\n",
    "\n",
    "ax.contour(X, Y, Z, levels=30, cmap='viridis', linewidths=0.8)\n",
    "ax.contourf(X, Y, Z, levels=30, cmap='viridis', alpha=0.3)\n",
    "\n",
    "# Trajectoires avec couleurs diff√©rentes\n",
    "colors = ['green', 'orange', 'red', 'darkred']\n",
    "for (name, traj), color in zip(trajectories_fail1.items(), colors):\n",
    "    if len(traj) > 1 and not np.isnan(traj[0][0]):\n",
    "        # Limiter l'affichage aux 10 premiers points si divergence\n",
    "        display_traj = traj[:min(10, len(traj))]\n",
    "        ax.plot(display_traj[:, 0], display_traj[:, 1], \n",
    "               color=color, linewidth=2, marker='o', markersize=4,\n",
    "               label=f'{name} ({len(traj)} it√©r.)')\n",
    "        \n",
    "        # Marquer le d√©but et la fin\n",
    "        ax.plot(traj[0, 0], traj[0, 1], 'o', color=color, \n",
    "               markersize=10, markeredgecolor='black', markeredgewidth=2)\n",
    "        if len(traj) > 1:\n",
    "            ax.plot(display_traj[-1, 0], display_traj[-1, 1], 's', \n",
    "                   color=color, markersize=10, markeredgecolor='black', \n",
    "                   markeredgewidth=2)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('√âchec 1 : Impact du Learning Rate (Divergence)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.savefig('echec1_lr_divergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Œ±=0.1 (vert) : Converge normalement vers (0,0)\n",
    "# - Œ±=0.5 (orange) : Oscille un peu mais converge\n",
    "# - Œ±=1.0 (rouge) : DIVERGE ! Part vers l'infini, trajectoire qui s'√©loigne\n",
    "# - Œ±=1.5 (rouge fonc√©) : EXPLOSE encore plus vite\n",
    "# - Message console montre clairement les divergences\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Cas o√π √ßa NE MARCHE PAS (divergence)\n",
    "# ‚úì D√©crire POURQUOI √ßa √©choue (learning rate trop grand)\n",
    "# ‚úì Visualisation claire du probl√®me\n",
    "\n",
    "# Graphe de convergence\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "for (name, costs), color in zip(costs_fail1.items(), colors):\n",
    "    if not np.isnan(costs[0]):\n",
    "        # Limiter aux 30 premi√®res it√©rations pour voir la divergence\n",
    "        display_costs = costs[:min(30, len(costs))]\n",
    "        ax.plot(display_costs, color=color, linewidth=2, label=name, marker='o')\n",
    "\n",
    "ax.set_xlabel('It√©ration', fontsize=12)\n",
    "ax.set_ylabel('Co√ªt f(x, y)', fontsize=12)\n",
    "ax.set_title('Divergence : Co√ªt qui MONTE au lieu de descendre', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')  # √âchelle log pour voir les explosions\n",
    "plt.tight_layout()\n",
    "plt.savefig('echec1_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Œ±=0.1 : Courbe qui DESCEND (bon comportement)\n",
    "# - Œ±=0.5 : Quelques oscillations puis descend\n",
    "# - Œ±=1.0 : Courbe qui MONTE ! (√©chec clair)\n",
    "# - Œ±=1.5 : Monte encore plus vite\n",
    "# - En √©chelle log, on voit bien l'explosion exponentielle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Learning Rate trop petit : Stagnation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"CAS D'√âCHEC 2 : Learning Rate Trop Petit ‚Üí Stagnation\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Rosenbrock avec learning rate minuscule\n",
    "x0 = np.array([-1.0, 1.0])\n",
    "learning_rates_slow = [0.001, 0.0001, 0.00001]\n",
    "\n",
    "trajectories_fail2 = {}\n",
    "costs_fail2 = {}\n",
    "\n",
    "for lr in learning_rates_slow:\n",
    "    traj, costs = gradient_descent(\n",
    "        rosenbrock, grad_rosenbrock, x0,\n",
    "        learning_rate=lr, max_iter=1000\n",
    "    )\n",
    "    trajectories_fail2[f'Œ±={lr}'] = traj\n",
    "    costs_fail2[f'Œ±={lr}'] = costs\n",
    "    \n",
    "    # Diagnostic\n",
    "    distance_to_optimum = np.linalg.norm(traj[-1] - np.array([1.0, 1.0]))\n",
    "    progress = costs[0] - costs[-1]\n",
    "    \n",
    "    print(f\"Œ±={lr} : {len(traj)} it√©rations\")\n",
    "    print(f\"  Co√ªt initial : {costs[0]:.4f}\")\n",
    "    print(f\"  Co√ªt final   : {costs[-1]:.4f}\")\n",
    "    print(f\"  Progr√®s      : {progress:.4f}\")\n",
    "    print(f\"  Distance √† (1,1) : {distance_to_optimum:.4f}\")\n",
    "    \n",
    "    if distance_to_optimum > 0.1:\n",
    "        print(f\"  ‚ùå √âCHEC : N'a pas atteint le minimum\")\n",
    "    elif len(traj) >= 1000:\n",
    "        print(f\"  ‚ö†Ô∏è  LENT : A atteint max_iter\")\n",
    "    else:\n",
    "        print(f\"  ‚úÖ OK mais lent\")\n",
    "    print()\n",
    "\n",
    "# Graphe de comparaison\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "x = np.linspace(-2, 2, 200)\n",
    "y = np.linspace(-1, 3, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = rosenbrock(np.array([X[i, j], Y[i, j]]))\n",
    "\n",
    "ax.contour(X, Y, Z, levels=50, cmap='viridis', linewidths=0.8)\n",
    "ax.contourf(X, Y, Z, levels=50, cmap='viridis', alpha=0.3)\n",
    "\n",
    "colors = ['red', 'orange', 'yellow']\n",
    "for (name, traj), color in zip(trajectories_fail2.items(), colors):\n",
    "    # Afficher seulement chaque 10e point pour lisibilit√©\n",
    "    display_indices = range(0, len(traj), 10)\n",
    "    display_traj = traj[display_indices]\n",
    "    \n",
    "    ax.plot(display_traj[:, 0], display_traj[:, 1], \n",
    "           color=color, linewidth=2, alpha=0.7,\n",
    "           label=f'{name} ({len(traj)} it√©r.)')\n",
    "    \n",
    "    # D√©but et fin\n",
    "    ax.plot(traj[0, 0], traj[0, 1], 'go', markersize=10)\n",
    "    ax.plot(traj[-1, 0], traj[-1, 1], 'ro', markersize=8)\n",
    "\n",
    "# Marquer le vrai minimum\n",
    "ax.plot(1.0, 1.0, 'b*', markersize=20, label='Minimum global (1,1)')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('√âchec 2 : Learning Rate Trop Petit (Stagnation)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.savefig('echec2_lr_stagnation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - 3 trajectoires de couleurs diff√©rentes depuis (-1,1)\n",
    "# - Plus le learning rate est petit, plus la trajectoire est \"courte\" (peu de progr√®s)\n",
    "# - Œ±=0.00001 (jaune) : √Ä peine boug√© de (-1,1)\n",
    "# - Œ±=0.0001 (orange) : A avanc√© un peu mais loin de (1,1)\n",
    "# - Œ±=0.001 (rouge) : Meilleur progr√®s mais encore insuffisant\n",
    "# - AUCUN n'atteint l'√©toile bleue en (1,1)\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Cas o√π √ßa NE MARCHE PAS (trop lent)\n",
    "# ‚úì Illustre les PLATEAUX (Rosenbrock)\n",
    "# ‚úì Montre l'importance du choix du learning rate\n",
    "\n",
    "# Graphe de convergence\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "for (name, costs), color in zip(costs_fail2.items(), colors):\n",
    "    ax.plot(costs, color=color, linewidth=2, label=name)\n",
    "\n",
    "ax.set_xlabel('It√©ration', fontsize=12)\n",
    "ax.set_ylabel('Co√ªt f(x, y)', fontsize=12)\n",
    "ax.set_title('Stagnation : Descente Extr√™mement Lente', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.savefig('echec2_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - 3 courbes qui descendent TR√àS lentement\n",
    "# - Œ±=0.00001 : Presque plate, descente imperceptible\n",
    "# - Œ±=0.0001 : Descend un peu mais reste haut\n",
    "# - Œ±=0.001 : Meilleure descente mais loin de 0\n",
    "# - Toutes finissent loin de 10‚Åª‚Å∂ (objectif normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Minimum Local : Pi√®ge d'Ackley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"CAS D'√âCHEC 3 : Minimum Local (Pi√®ge d'Ackley)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Tester Ackley avec plusieurs points de d√©part\n",
    "starting_points = [\n",
    "    (\"Proche (1,1)\", np.array([1.0, 1.0])),\n",
    "    (\"Moyen (3,3)\", np.array([3.0, 3.0])),\n",
    "    (\"Loin (5,5)\", np.array([5.0, 5.0]))\n",
    "]\n",
    "\n",
    "trajectories_fail3 = {}\n",
    "costs_fail3 = {}\n",
    "\n",
    "for name, x0 in starting_points:\n",
    "    traj, costs = gradient_descent(\n",
    "        ackley, grad_ackley, x0,\n",
    "        learning_rate=0.01, max_iter=500\n",
    "    )\n",
    "    trajectories_fail3[name] = traj\n",
    "    costs_fail3[name] = costs\n",
    "    \n",
    "    # Diagnostic\n",
    "    final_point = traj[-1]\n",
    "    final_cost = costs[-1]\n",
    "    distance_to_global = np.linalg.norm(final_point)\n",
    "    \n",
    "    print(f\"{name} : d√©part {x0}\")\n",
    "    print(f\"  Arriv√©e : ({final_point[0]:.4f}, {final_point[1]:.4f})\")\n",
    "    print(f\"  Co√ªt final : {final_cost:.6f}\")\n",
    "    print(f\"  Distance √† (0,0) : {distance_to_global:.4f}\")\n",
    "    \n",
    "    if final_cost < 0.1:\n",
    "        print(f\"  ‚úÖ SUCC√àS : A trouv√© le minimum global\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå √âCHEC : Bloqu√© dans un minimum local\")\n",
    "    print()\n",
    "\n",
    "# Graphe de comparaison\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "x = np.linspace(-6, 6, 300)\n",
    "y = np.linspace(-6, 6, 300)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = ackley(np.array([X[i, j], Y[i, j]]))\n",
    "\n",
    "ax.contour(X, Y, Z, levels=50, cmap='viridis', linewidths=0.8)\n",
    "ax.contourf(X, Y, Z, levels=50, cmap='viridis', alpha=0.3)\n",
    "\n",
    "colors = ['green', 'orange', 'red']\n",
    "for (name, traj), color in zip(trajectories_fail3.items(), colors):\n",
    "    ax.plot(traj[:, 0], traj[:, 1], \n",
    "           color=color, linewidth=2, alpha=0.8,\n",
    "           label=f'{name} ({len(traj)} it√©r.)')\n",
    "    \n",
    "    # D√©but et fin\n",
    "    ax.plot(traj[0, 0], traj[0, 1], 'o', color=color, markersize=10,\n",
    "           markeredgecolor='black', markeredgewidth=2)\n",
    "    ax.plot(traj[-1, 0], traj[-1, 1], 's', color=color, markersize=8,\n",
    "           markeredgecolor='black', markeredgewidth=2)\n",
    "\n",
    "# Marquer le minimum global\n",
    "ax.plot(0.0, 0.0, 'b*', markersize=25, label='Minimum GLOBAL (0,0)',\n",
    "       markeredgecolor='white', markeredgewidth=2)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('√âchec 3 : Pi√®ge des Minima Locaux (Ackley)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.savefig('echec3_minima_locaux.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Point vert : Part de (1,1), ATTEINT l'√©toile bleue (succ√®s)\n",
    "# - Point orange : Part de (3,3), se BLOQUE dans un minimum local proche\n",
    "# - Point rouge : Part de (5,5), se BLOQUE encore plus loin\n",
    "# - Les points orange et rouge finissent dans des \"trous\" locaux, pas au centre\n",
    "# - Illustration parfaite du probl√®me : point initial d√©termine le succ√®s\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Illustre les MINIMA LOCAUX (centaines dans Ackley)\n",
    "# ‚úì R√¥le du POINT INITIAL (crucial !)\n",
    "# ‚úì Cas d'√©chec m√™me avec bon algorithme\n",
    "\n",
    "# Graphe de convergence\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "for (name, costs), color in zip(costs_fail3.items(), colors):\n",
    "    ax.plot(costs, color=color, linewidth=2, label=name, marker='o', markersize=3)\n",
    "\n",
    "# Ligne pour montrer le seuil de \"succ√®s\"\n",
    "ax.axhline(y=0.1, color='blue', linestyle='--', linewidth=2, \n",
    "          label='Seuil succ√®s (f<0.1)')\n",
    "\n",
    "ax.set_xlabel('It√©ration', fontsize=12)\n",
    "ax.set_ylabel('Co√ªt f(x, y)', fontsize=12)\n",
    "ax.set_title('Convergence : Global vs Locaux', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.savefig('echec3_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Courbe verte : Descend jusqu'√† ~10‚Åª¬π‚Å∞ (sous le seuil bleu) ‚úÖ\n",
    "# - Courbe orange : Descend puis SE STABILISE √† ~1-3 (au-dessus du seuil) ‚ùå\n",
    "# - Courbe rouge : Descend puis SE STABILISE encore plus haut ‚ùå\n",
    "# - C'est CLAIR visuellement qui a r√©ussi vs √©chou√©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Momentum trop √©lev√© : Oscillations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"CAS D'√âCHEC 4 : Momentum trop √©lev√© ‚Üí Oscillations\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Tester diff√©rents momentum sur quadratique\n",
    "x0 = np.array([5.0, 5.0])\n",
    "momentum_values = [0.5, 0.9, 0.95, 0.99]\n",
    "\n",
    "trajectories_fail4 = {}\n",
    "costs_fail4 = {}\n",
    "\n",
    "for beta in momentum_values:\n",
    "    traj, costs = gradient_descent_momentum(\n",
    "        quadratique, grad_quadratique, x0,\n",
    "        learning_rate=0.1, momentum=beta, max_iter=200\n",
    "    )\n",
    "    trajectories_fail4[f'Œ≤={beta}'] = traj\n",
    "    costs_fail4[f'Œ≤={beta}'] = costs\n",
    "    \n",
    "    # Diagnostic\n",
    "    final_cost = costs[-1]\n",
    "    # Compter les oscillations (co√ªt qui monte puis descend)\n",
    "    oscillations = sum(1 for i in range(1, len(costs)-1) \n",
    "                      if costs[i] > costs[i-1] and costs[i] > costs[i+1])\n",
    "    \n",
    "    print(f\"Œ≤={beta} : {len(traj)} it√©rations, {oscillations} oscillations\")\n",
    "    print(f\"  Co√ªt final : {final_cost:.2e}\")\n",
    "    \n",
    "    if final_cost < 1e-6:\n",
    "        if oscillations > 10:\n",
    "            print(f\"  ‚ö†Ô∏è  Converge mais avec beaucoup d'oscillations\")\n",
    "        else:\n",
    "            print(f\"  ‚úÖ Converge normalement\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå N'a pas converg√© correctement\")\n",
    "    print()\n",
    "\n",
    "# Graphe de comparaison\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "x = np.linspace(-6, 6, 200)\n",
    "y = np.linspace(-6, 6, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = quadratique(np.array([X[i, j], Y[i, j]]))\n",
    "\n",
    "ax.contour(X, Y, Z, levels=30, cmap='viridis', linewidths=0.8)\n",
    "ax.contourf(X, Y, Z, levels=30, cmap='viridis', alpha=0.3)\n",
    "\n",
    "colors = ['green', 'blue', 'orange', 'red']\n",
    "for (name, traj), color in zip(trajectories_fail4.items(), colors):\n",
    "    ax.plot(traj[:, 0], traj[:, 1], \n",
    "           color=color, linewidth=1.5, alpha=0.7,\n",
    "           label=f'{name} ({len(traj)} it√©r.)')\n",
    "    \n",
    "    # D√©but\n",
    "    ax.plot(traj[0, 0], traj[0, 1], 'go', markersize=10)\n",
    "    \n",
    "    # Dernier point\n",
    "    ax.plot(traj[-1, 0], traj[-1, 1], 'o', color=color, markersize=8)\n",
    "\n",
    "ax.plot(0, 0, 'b*', markersize=20, label='Optimum (0,0)')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('√âchec 4 : Momentum Excessif (Oscillations)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.savefig('echec4_momentum_oscillations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Œ≤=0.5 (vert) : Trajectoire relativement lisse vers (0,0)\n",
    "# - Œ≤=0.9 (bleu) : L√©g√®rement plus d'oscillations mais OK\n",
    "# - Œ≤=0.95 (orange) : BEAUCOUP d'oscillations, d√©passe le minimum\n",
    "# - Œ≤=0.99 (rouge) : OSCILLATIONS EXTR√äMES, fait des grands allers-retours\n",
    "# - Plus le momentum est √©lev√©, plus la trajectoire \"serpente\"\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì R√¥le du param√®tre MOMENTUM\n",
    "# ‚úì Cas d'√©chec : momentum trop grand ‚Üí instabilit√©\n",
    "\n",
    "# Graphe de convergence\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "for (name, costs), color in zip(costs_fail4.items(), colors):\n",
    "    ax.plot(costs, color=color, linewidth=2, label=name, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('It√©ration', fontsize=12)\n",
    "ax.set_ylabel('Co√ªt f(x, y)', fontsize=12)\n",
    "ax.set_title('Oscillations dues au Momentum Excessif', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.savefig('echec4_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Œ≤=0.5 : Descente lisse\n",
    "# - Œ≤=0.9 : Descente avec petites vagues\n",
    "# - Œ≤=0.95 : Descente avec GROSSES vagues (co√ªt monte et descend)\n",
    "# - Œ≤=0.99 : Vagues √âNORMES, peut m√™me remonter temporairement\n",
    "# - En √©chelle log, on voit clairement les oscillations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Ravine √©troite : Zigzags Extr√™mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"CAS D'√âCHEC 5 : Ravine √âtroite ‚Üí Zigzags Extr√™mes\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Fonction quadratique tr√®s mal conditionn√©e\n",
    "def quadratique_extreme(x):\n",
    "    \"\"\"f(x,y) = x¬≤ + 100y¬≤ - ravine tr√®s √©troite selon x\"\"\"\n",
    "    return x[0]**2 + 100*x[1]**2\n",
    "\n",
    "def grad_quadratique_extreme(x):\n",
    "    return np.array([2*x[0], 200*x[1]])\n",
    "\n",
    "x0 = np.array([10.0, 10.0])\n",
    "\n",
    "# Comparer Simple vs Momentum\n",
    "traj_simple_zig, costs_simple_zig = gradient_descent(\n",
    "    quadratique_extreme, grad_quadratique_extreme, x0,\n",
    "    learning_rate=0.01, max_iter=300\n",
    ")\n",
    "\n",
    "traj_momentum_zig, costs_momentum_zig = gradient_descent_momentum(\n",
    "    quadratique_extreme, grad_quadratique_extreme, x0,\n",
    "    learning_rate=0.01, momentum=0.9, max_iter=300\n",
    ")\n",
    "\n",
    "print(f\"Simple : {len(traj_simple_zig)} it√©rations\")\n",
    "print(f\"  Co√ªt final : {costs_simple_zig[-1]:.2e}\")\n",
    "\n",
    "print(f\"\\nMomentum : {len(traj_momentum_zig)} it√©rations\")\n",
    "print(f\"  Co√ªt final : {costs_momentum_zig[-1]:.2e}\")\n",
    "\n",
    "# Graphe de comparaison\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "x = np.linspace(-12, 12, 200)\n",
    "y = np.linspace(-12, 12, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = quadratique_extreme(np.array([X[i, j], Y[i, j]]))\n",
    "\n",
    "ax.contour(X, Y, Z, levels=30, cmap='viridis', linewidths=0.8)\n",
    "ax.contourf(X, Y, Z, levels=30, cmap='viridis', alpha=0.3)\n",
    "\n",
    "# Simple en rouge (zigzags)\n",
    "ax.plot(traj_simple_zig[:, 0], traj_simple_zig[:, 1], \n",
    "       'r-', linewidth=1.5, alpha=0.7, label=f'Simple ({len(traj_simple_zig)} it√©r.)')\n",
    "\n",
    "# Momentum en bleu (plus lisse)\n",
    "ax.plot(traj_momentum_zig[:, 0], traj_momentum_zig[:, 1], \n",
    "       'b-', linewidth=1.5, alpha=0.7, label=f'Momentum ({len(traj_momentum_zig)} it√©r.)')\n",
    "\n",
    "# Points de d√©part et arriv√©e\n",
    "ax.plot(10, 10, 'go', markersize=10, label='D√©part')\n",
    "ax.plot(0, 0, 'b*', markersize=20, label='Optimum (0,0)')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('√âchec 5 : Zigzags dans Ravine √âtroite (ratio 1:100)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.savefig('echec5_zigzags_ravine.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Ellipses TR√àS √©tir√©es (ratio 1:100)\n",
    "# - Simple (rouge) : ZIGZAGS marqu√©s perpendiculaires √† la direction du minimum\n",
    "# - Momentum (bleu) : Trajectoire BEAUCOUP plus lisse\n",
    "# - Les deux partent de (10,10) et visent (0,0)\n",
    "# - Diff√©rence spectaculaire : zigzags vs ligne relativement directe\n",
    "#\n",
    "# üìã CAHIER DES CHARGES :\n",
    "# ‚úì Illustre les RAVINES (fonction mal conditionn√©e)\n",
    "# ‚úì Montre pourquoi Momentum am√©liore Simple\n",
    "# ‚úì Cas o√π Simple est tr√®s inefficace\n",
    "\n",
    "# Graphe de convergence\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "ax.plot(costs_simple_zig, 'r-', linewidth=2, label='Simple', alpha=0.7)\n",
    "ax.plot(costs_momentum_zig, 'b-', linewidth=2, label='Momentum', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('It√©ration', fontsize=12)\n",
    "ax.set_ylabel('Co√ªt f(x, y)', fontsize=12)\n",
    "ax.set_title('Convergence : Simple vs Momentum dans Ravine', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.savefig('echec5_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ R√âSULTAT ATTENDU :\n",
    "# - Simple : Descente en \"escalier\" (oscillations)\n",
    "# - Momentum : Descente beaucoup plus lisse et rapide\n",
    "# - Momentum converge ~2-3x plus vite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tableau r√©capitulatif des √©checs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TABLEAU R√âCAPITULATIF DES CAS D'√âCHEC\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_echecs = {\n",
    "    'Cas d\\'√âchec': [\n",
    "        '1. LR trop grand',\n",
    "        '2. LR trop petit',\n",
    "        '3. Minimum local',\n",
    "        '4. Momentum excessif',\n",
    "        '5. Ravine √©troite'\n",
    "    ],\n",
    "    'Sympt√¥me': [\n",
    "        'Divergence, explosion',\n",
    "        'Stagnation, progression lente',\n",
    "        'Converge mais pas au global',\n",
    "        'Oscillations persistantes',\n",
    "        'Zigzags inefficaces'\n",
    "    ],\n",
    "    'Cause': [\n",
    "        'Œ± > seuil stabilit√©',\n",
    "        'Œ± << gradient',\n",
    "        'Point initial √©loign√©',\n",
    "        'Œ≤ proche de 1.0',\n",
    "        'Fonction mal conditionn√©e'\n",
    "    ],\n",
    "    'Solution': [\n",
    "        'R√©duire Œ± ou use Adam',\n",
    "        'Augmenter Œ± prudemment',\n",
    "        'Momentum/Adam ou restart',\n",
    "        'R√©duire Œ≤ (0.9 typique)',\n",
    "        'Momentum ou Adam'\n",
    "    ],\n",
    "    'Graphe': [\n",
    "        'echec1_lr_divergence.png',\n",
    "        'echec2_lr_stagnation.png',\n",
    "        'echec3_minima_locaux.png',\n",
    "        'echec4_momentum_oscillations.png',\n",
    "        'echec5_zigzags_ravine.png'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_echecs = pd.DataFrame(data_echecs)\n",
    "print(df_echecs.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOTAL : 10 graphes d'√©checs g√©n√©r√©s (2 par cas)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Notes pour le rapport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OBSERVATIONS POUR LE RAPPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "1. FONCTION QUADRATIQUE\n",
    "   - Tous les algorithmes convergent\n",
    "   - Simple : zigzags visibles dus √† la diff√©rence de courbure\n",
    "   - Momentum/Nesterov : trajectoires plus lisses\n",
    "   - Adam : convergence la plus rapide\n",
    "\n",
    "2. ROSENBROCK\n",
    "   - Fonction tr√®s difficile (vall√©e √©troite)\n",
    "   - Simple/Momentum/Nesterov : tr√®s lents\n",
    "   - Adam : beaucoup plus efficace gr√¢ce √† l'adaptation du learning rate\n",
    "\n",
    "3. BOOTH\n",
    "   - Convergence rapide pour tous les algorithmes\n",
    "   - Paysage relativement simple\n",
    "\n",
    "4. BEALE\n",
    "   - Fonction avec de forts gradients pr√®s de l'origine\n",
    "   - N√©cessite un learning rate plus petit\n",
    "   - Adam s'adapte automatiquement\n",
    "\n",
    "5. HIMMELBLAU\n",
    "   - 4 minima globaux √©quivalents\n",
    "   - Le choix du point de d√©part d√©termine quel minimum est atteint\n",
    "   - Tous les algorithmes convergent vers un des minima\n",
    "\n",
    "CONCLUSION :\n",
    "- Simple : fonctionne mais lent et zigzague\n",
    "- Momentum : am√©liore Simple mais sensible aux param√®tres\n",
    "- Nesterov : l√©g√®rement meilleur que Momentum\n",
    "- Adam : le plus robuste, s'adapte automatiquement\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
