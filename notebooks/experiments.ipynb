{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration matplotlib pour les graphes\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['legend.fontsize'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Définition des fonctions de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratique(x):\n",
    "    \"\"\"f(x, y) = x² + 2y²\"\"\"\n",
    "    return x[0]**2 + 2*x[1]**2\n",
    "\n",
    "def grad_quadratique(x):\n",
    "    \"\"\"Gradient analytique de la fonction quadratique\"\"\"\n",
    "    return np.array([2*x[0], 4*x[1]])\n",
    "\n",
    "\n",
    "def rosenbrock(x):\n",
    "    \"\"\"f(x, y) = (1-x)² + 100(y-x²)²\n",
    "    Minimum global : (1, 1) avec f(1,1) = 0\n",
    "    \"\"\"\n",
    "    return (1 - x[0])**2 + 100*(x[1] - x[0]**2)**2\n",
    "\n",
    "def grad_rosenbrock(x):\n",
    "    \"\"\"Gradient analytique de Rosenbrock\"\"\"\n",
    "    gx = -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2)\n",
    "    gy = 200*(x[1] - x[0]**2)\n",
    "    return np.array([gx, gy])\n",
    "\n",
    "\n",
    "def booth(x):\n",
    "    \"\"\"f(x, y) = (x + 2y - 7)² + (2x + y - 5)²\n",
    "    Minimum global : (1, 3) avec f(1,3) = 0\n",
    "    \"\"\"\n",
    "    return (x[0] + 2*x[1] - 7)**2 + (2*x[0] + x[1] - 5)**2\n",
    "\n",
    "def grad_booth(x):\n",
    "    \"\"\"Gradient analytique de Booth\"\"\"\n",
    "    gx = 2*(x[0] + 2*x[1] - 7) + 4*(2*x[0] + x[1] - 5)\n",
    "    gy = 4*(x[0] + 2*x[1] - 7) + 2*(2*x[0] + x[1] - 5)\n",
    "    return np.array([gx, gy])\n",
    "\n",
    "\n",
    "def beale(x):\n",
    "    \"\"\"f(x, y) = (1.5 - x + xy)² + (2.25 - x + xy²)² + (2.625 - x + xy³)²\n",
    "    Minimum global : (3, 0.5) avec f(3,0.5) = 0\n",
    "    \"\"\"\n",
    "    term1 = (1.5 - x[0] + x[0]*x[1])**2\n",
    "    term2 = (2.25 - x[0] + x[0]*x[1]**2)**2\n",
    "    term3 = (2.625 - x[0] + x[0]*x[1]**3)**2\n",
    "    return term1 + term2 + term3\n",
    "\n",
    "def grad_beale(x):\n",
    "    \"\"\"Gradient numérique de Beale\"\"\"\n",
    "    h = 1e-5\n",
    "    gx = (beale(x + np.array([h, 0])) - beale(x - np.array([h, 0]))) / (2*h)\n",
    "    gy = (beale(x + np.array([0, h])) - beale(x - np.array([0, h]))) / (2*h)\n",
    "    return np.array([gx, gy])\n",
    "\n",
    "\n",
    "def himmelblau(x):\n",
    "    \"\"\"f(x, y) = (x² + y - 11)² + (x + y² - 7)²\n",
    "    4 minima globaux : (3, 2), (-2.805, 3.131), (-3.779, -3.283), (3.584, -1.848)\n",
    "    Tous avec f = 0\n",
    "    \"\"\"\n",
    "    return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2\n",
    "\n",
    "def grad_himmelblau(x):\n",
    "    \"\"\"Gradient analytique de Himmelblau\"\"\"\n",
    "    gx = 4*x[0]*(x[0]**2 + x[1] - 11) + 2*(x[0] + x[1]**2 - 7)\n",
    "    gy = 2*(x[0]**2 + x[1] - 11) + 4*x[1]*(x[0] + x[1]**2 - 7)\n",
    "    return np.array([gx, gy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Algorithmes d'optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, grad_f, x0, learning_rate=0.01, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Descente de gradient simple.\n",
    "    \n",
    "    Formule : x_new = x - learning_rate × ∇f(x)\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    trajectory = [x.copy()]\n",
    "    costs = [f(x)]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "\n",
    "        # DEBUG : Afficher la norme du gradient\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        if i < 5 or i % 100 == 0:  # Afficher les premières itérations\n",
    "            print(f\"  Iter {i}: grad_norm = {grad_norm:.2e}, cost = {costs[-1]:.6f}\")\n",
    "        \n",
    "        # Critère d'arrêt : gradient très petit et coût très petit\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        \n",
    "        # Mise à jour\n",
    "        x = x - learning_rate * grad\n",
    "        trajectory.append(x.copy())\n",
    "        costs.append(f(x))\n",
    "    \n",
    "    return np.array(trajectory), np.array(costs)\n",
    "\n",
    "\n",
    "def gradient_descent_momentum(f, grad_f, x0, learning_rate=0.01, momentum=0.8, \n",
    "                               max_iter=1000, tol=1e-6):    \n",
    "    \"\"\"\n",
    "    Descente de gradient avec Momentum.\n",
    "    \n",
    "    Formule : \n",
    "        v = momentum × v + learning_rate × ∇f(x)\n",
    "        x_new = x - v\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    v = np.zeros_like(x)\n",
    "    trajectory = [x.copy()]\n",
    "    costs = [f(x)]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        \n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        \n",
    "        # Mise à jour de la vitesse\n",
    "        v = momentum * v + learning_rate * grad\n",
    "        \n",
    "        # Mise à jour de la position\n",
    "        x = x - v\n",
    "        trajectory.append(x.copy())\n",
    "        costs.append(f(x))\n",
    "    \n",
    "    return np.array(trajectory), np.array(costs)\n",
    "\n",
    "\n",
    "def gradient_descent_nesterov(f, grad_f, x0, learning_rate=0.01, momentum=0.8,\n",
    "                               max_iter=1000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Descente de gradient Nesterov (NAG).\n",
    "    \n",
    "    Formule :\n",
    "        x_lookahead = x - momentum × v\n",
    "        v = momentum × v + learning_rate × ∇f(x_lookahead)\n",
    "        x_new = x - v\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    v = np.zeros_like(x)\n",
    "    trajectory = [x.copy()]\n",
    "    costs = [f(x)]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # Point anticipé\n",
    "        x_lookahead = x - momentum * v\n",
    "        grad = grad_f(x_lookahead)\n",
    "        \n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        \n",
    "        # Mise à jour de la vitesse\n",
    "        v = momentum * v + learning_rate * grad\n",
    "        \n",
    "        # Mise à jour de la position\n",
    "        x = x - v\n",
    "        trajectory.append(x.copy())\n",
    "        costs.append(f(x))\n",
    "    \n",
    "    return np.array(trajectory), np.array(costs)\n",
    "\n",
    "\n",
    "def gradient_descent_adam(f, grad_f, x0, learning_rate=0.01, beta1=0.9, beta2=0.999,\n",
    "                          epsilon=1e-8, max_iter=1000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Algorithme Adam (Adaptive Moment Estimation).\n",
    "    \n",
    "    Formule :\n",
    "        m = beta1 × m + (1-beta1) × ∇f(x)\n",
    "        v = beta2 × v + (1-beta2) × (∇f(x))²\n",
    "        m_hat = m / (1 - beta1^t)\n",
    "        v_hat = v / (1 - beta2^t)\n",
    "        x_new = x - learning_rate × m_hat / (√v_hat + epsilon)\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    m = np.zeros_like(x)\n",
    "    v = np.zeros_like(x)\n",
    "    trajectory = [x.copy()]\n",
    "    costs = [f(x)]\n",
    "    \n",
    "    for t in range(1, max_iter + 1):\n",
    "        grad = grad_f(x)\n",
    "        \n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        \n",
    "        # Mise à jour des moments\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "        \n",
    "        # Correction du biais\n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        \n",
    "        # Mise à jour de la position\n",
    "        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        trajectory.append(x.copy())\n",
    "        costs.append(f(x))\n",
    "    \n",
    "    return np.array(trajectory), np.array(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fonctions de visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory_2d(f, trajectory, costs, x_range, y_range, \n",
    "                       title=\"\", algo_name=\"\", num_levels=30, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    Graphe professionnel avec courbes de niveau et trajectoire.\n",
    "    Style épuré et clean.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Grille pour les courbes de niveau\n",
    "    x = np.linspace(x_range[0], x_range[1], 200)\n",
    "    y = np.linspace(y_range[0], y_range[1], 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Calcul de Z\n",
    "    Z = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Z[i, j] = f(np.array([X[i, j], Y[i, j]]))\n",
    "    \n",
    "    # Courbes de niveau avec dégradé\n",
    "    ax.contour(X, Y, Z, levels=num_levels, cmap='viridis', linewidths=0.6, alpha=0.8)\n",
    "    ax.contourf(X, Y, Z, levels=num_levels, cmap='viridis', alpha=0.25)\n",
    "    \n",
    "    # Sous-échantillonner si trop long\n",
    "    if len(trajectory) > 300:\n",
    "        indices = np.linspace(0, len(trajectory)-1, 300, dtype=int)\n",
    "        display_traj = trajectory[indices]\n",
    "    else:\n",
    "        display_traj = trajectory\n",
    "    \n",
    "    # Trajectoire - couleur douce\n",
    "    ax.plot(display_traj[:, 0], display_traj[:, 1], '-', color='#E07A5F', \n",
    "            linewidth=1.8, label=f'{algo_name} ({len(trajectory)} itér.)', alpha=0.85)\n",
    "    \n",
    "    # Points discrets : départ et arrivée (petits, sans contour)\n",
    "    ax.plot(trajectory[0, 0], trajectory[0, 1], 'o', color='#457B9D', \n",
    "            markersize=5, label='Départ', zorder=5)\n",
    "    ax.plot(trajectory[-1, 0], trajectory[-1, 1], 'o', color='#E63946', \n",
    "            markersize=5, label='Arrivée', zorder=5)\n",
    "    \n",
    "    # Mise en forme épurée\n",
    "    ax.set_xlabel('x', fontsize=11)\n",
    "    ax.set_ylabel('y', fontsize=11)\n",
    "    ax.set_title(title, fontsize=13, fontweight='medium')\n",
    "    ax.legend(fontsize=9, loc='best', framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.2, linestyle='-', linewidth=0.5)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_comparison_trajectories(f, trajectories_dict, x_range, y_range,\n",
    "                                 title=\"\", num_levels=30, figsize=(12, 9)):\n",
    "    \"\"\"\n",
    "    Compare plusieurs algorithmes sur le même graphe.\n",
    "    Style épuré avec couleurs pastel.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Grille et courbes de niveau\n",
    "    x = np.linspace(x_range[0], x_range[1], 200)\n",
    "    y = np.linspace(y_range[0], y_range[1], 200)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    Z = np.zeros_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            Z[i, j] = f(np.array([X[i, j], Y[i, j]]))\n",
    "    \n",
    "    # Courbes de niveau\n",
    "    ax.contour(X, Y, Z, levels=num_levels, cmap='viridis', linewidths=0.6, alpha=0.8)\n",
    "    ax.contourf(X, Y, Z, levels=num_levels, cmap='viridis', alpha=0.2)\n",
    "    \n",
    "    # Couleurs pastel professionnelles\n",
    "    colors = ['#E07A5F', '#457B9D', '#6A994E', '#E9A319']  # Terracotta, Steel Blue, Fern, Marigold\n",
    "    \n",
    "    for idx, (algo_name, trajectory) in enumerate(trajectories_dict.items()):\n",
    "        color = colors[idx % len(colors)]\n",
    "        \n",
    "        # Sous-échantillonner les longues trajectoires\n",
    "        if len(trajectory) > 300:\n",
    "            indices = np.linspace(0, len(trajectory)-1, 300, dtype=int)\n",
    "            display_traj = trajectory[indices]\n",
    "        else:\n",
    "            display_traj = trajectory\n",
    "        \n",
    "        # Trajectoire\n",
    "        ax.plot(display_traj[:, 0], display_traj[:, 1], '-', color=color, \n",
    "                linewidth=1.8, label=f'{algo_name} ({len(trajectory)} itér.)', alpha=0.85)\n",
    "        \n",
    "        # Points discrets : départ (rond) et arrivée (carré) - petits, sans contour\n",
    "        ax.plot(trajectory[0, 0], trajectory[0, 1], 'o', color=color, \n",
    "                markersize=4, zorder=5)\n",
    "        ax.plot(trajectory[-1, 0], trajectory[-1, 1], 's', color=color, \n",
    "                markersize=4, zorder=5)\n",
    "    \n",
    "    ax.set_xlabel('x', fontsize=11)\n",
    "    ax.set_ylabel('y', fontsize=11)\n",
    "    ax.set_title(title, fontsize=13, fontweight='medium')\n",
    "    ax.legend(fontsize=9, loc='best', framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.2, linestyle='-', linewidth=0.5)\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_convergence_curves(costs_dict, title=\"Convergence\", figsize=(10, 6)):\n",
    "    \"\"\"\n",
    "    Graphe de convergence avec style épuré.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Couleurs pastel\n",
    "    colors = ['#E07A5F', '#457B9D', '#6A994E', '#E9A319']\n",
    "    \n",
    "    for idx, (algo_name, algo_costs) in enumerate(costs_dict.items()):\n",
    "        costs_array = np.array(algo_costs)\n",
    "        \n",
    "        # Gérer les valeurs <= 0 pour l'échelle log\n",
    "        if np.any(costs_array > 0):\n",
    "            min_positive = np.min(costs_array[costs_array > 0])\n",
    "            valid_costs = np.maximum(costs_array, min_positive * 1e-3)\n",
    "        else:\n",
    "            valid_costs = np.abs(costs_array) + 1e-12\n",
    "        \n",
    "        iterations = range(len(valid_costs))\n",
    "        ax.plot(iterations, valid_costs, '-', color=colors[idx % len(colors)],\n",
    "                linewidth=1.8, label=f'{algo_name} ({len(algo_costs)} itér.)', alpha=0.85)\n",
    "    \n",
    "    ax.set_xlabel('Itération', fontsize=11)\n",
    "    ax.set_ylabel('Coût f(x, y)', fontsize=11)\n",
    "    ax.set_title(title, fontsize=13, fontweight='medium')\n",
    "    ax.legend(fontsize=9, framealpha=0.9)\n",
    "    ax.grid(True, alpha=0.2, linestyle='-', linewidth=0.5)\n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Expériences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Fonction quatratique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPÉRIENCE 1 : Fonction Quadratique f(x,y) = x² + 2y²\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Point de départ\n",
    "x0 = np.array([5.0, 5.0])\n",
    "\n",
    "# Exécution des algorithmes\n",
    "traj_simple, costs_simple = gradient_descent(\n",
    "    quadratique, grad_quadratique, x0, learning_rate=0.1\n",
    ")\n",
    "\n",
    "traj_momentum, costs_momentum = gradient_descent_momentum(\n",
    "    quadratique, grad_quadratique, x0, learning_rate=0.1, momentum=0.8\n",
    ")\n",
    "\n",
    "traj_nesterov, costs_nesterov = gradient_descent_nesterov(\n",
    "    quadratique, grad_quadratique, x0, learning_rate=0.1, momentum=0.8\n",
    ")\n",
    "\n",
    "traj_adam, costs_adam = gradient_descent_adam(\n",
    "    quadratique, grad_quadratique, x0, learning_rate=0.1\n",
    ")\n",
    "\n",
    "# Affichage des résultats\n",
    "print(f\"\\nSimple : {len(traj_simple)} itérations, f(x*) = {costs_simple[-1]:.10f}\")\n",
    "print(f\"Momentum : {len(traj_momentum)} itérations, f(x*) = {costs_momentum[-1]:.10f}\")\n",
    "print(f\"Nesterov : {len(traj_nesterov)} itérations, f(x*) = {costs_nesterov[-1]:.10f}\")\n",
    "print(f\"Adam : {len(traj_adam)} itérations, f(x*) = {costs_adam[-1]:.10f}\")\n",
    "\n",
    "# Graphe individuel : Simple\n",
    "fig1, _ = plot_trajectory_2d(\n",
    "    quadratique, traj_simple, costs_simple,\n",
    "    x_range=(-6, 6), y_range=(-6, 6),\n",
    "    title=\"Quadratique : Descente Simple\",\n",
    "    algo_name=\"Simple\"\n",
    ")\n",
    "plt.savefig('../figures/temp/quad_simple.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Graphe de comparaison\n",
    "trajectories = {\n",
    "    'Simple': traj_simple,\n",
    "    'Momentum': traj_momentum,\n",
    "    'Nesterov': traj_nesterov,\n",
    "    'Adam': traj_adam\n",
    "}\n",
    "\n",
    "fig2, _ = plot_comparison_trajectories(\n",
    "    quadratique, trajectories,\n",
    "    x_range=(-6, 6), y_range=(-6, 6),\n",
    "    title=\"Quadratique : Comparaison des Algorithmes\"\n",
    ")\n",
    "plt.savefig('../figures/temp/quad_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Courbes de convergence\n",
    "costs = {\n",
    "    'Simple': costs_simple,\n",
    "    'Momentum': costs_momentum,\n",
    "    'Nesterov': costs_nesterov,\n",
    "    'Adam': costs_adam\n",
    "}\n",
    "\n",
    "fig3, _ = plot_convergence_curves(\n",
    "    costs, title=\"Quadratique : Convergence\"\n",
    ")\n",
    "plt.savefig('../figures/temp/quad_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Fonctions Simples g et h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPÉRIENCE 4.2 : Fonctions Simples g(x,y) et h(x,y)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Définir les fonctions\n",
    "def g(x):\n",
    "    \"\"\"g(x,y) = 1 - exp(-10x² - y²)\"\"\"\n",
    "    return 1 - np.exp(-10*x[0]**2 - x[1]**2)\n",
    "\n",
    "def grad_g(x):\n",
    "    \"\"\"Gradient de g\"\"\"\n",
    "    exp_term = np.exp(-10*x[0]**2 - x[1]**2)\n",
    "    gx = 20*x[0] * exp_term\n",
    "    gy = 2*x[1] * exp_term\n",
    "    return np.array([gx, gy])\n",
    "\n",
    "def h(x):\n",
    "    \"\"\"h(x,y) = x²y - 2xy³ + 3xy + 4\"\"\"\n",
    "    return x[0]**2 * x[1] - 2*x[0]*x[1]**3 + 3*x[0]*x[1] + 4\n",
    "\n",
    "def grad_h(x):\n",
    "    \"\"\"Gradient de h\"\"\"\n",
    "    gx = 2*x[0]*x[1] - 2*x[1]**3 + 3*x[1]\n",
    "    gy = x[0]**2 - 6*x[0]*x[1]**2 + 3*x[0]\n",
    "    return np.array([gx, gy])\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# TEST 1 : Fonction g (plateau)\n",
    "# =========================================================================\n",
    "\n",
    "print(\"\\n--- Fonction g : 1 - exp(-10x² - y²) ---\")\n",
    "x0 = np.array([0.3, 0.3])  # Point où le gradient est significatif\n",
    "\n",
    "traj_simple_g, costs_simple_g = gradient_descent(\n",
    "    g, grad_g, x0, learning_rate=0.03, max_iter=500, tol=1e-6  # ← 0.03 pour trajectoires lisses\n",
    ")\n",
    "\n",
    "traj_momentum_g, costs_momentum_g = gradient_descent_momentum(\n",
    "    g, grad_g, x0, learning_rate=0.03, momentum=0.5, max_iter=500, tol=1e-6  # ← trajectoires lisses\n",
    ")\n",
    "\n",
    "traj_nesterov_g, costs_nesterov_g = gradient_descent_nesterov(\n",
    "    g, grad_g, x0, learning_rate=0.03, momentum=0.5, max_iter=500, tol=1e-6  # ← trajectoires lisses\n",
    ")\n",
    "\n",
    "traj_adam_g, costs_adam_g = gradient_descent_adam(\n",
    "    g, grad_g, x0, learning_rate=0.03, max_iter=500, tol=1e-6  # ← 0.03 pour trajectoires lisses\n",
    ")\n",
    "\n",
    "print(f\"\\nSimple : {len(traj_simple_g)} itérations, f(x*) = {costs_simple_g[-1]:.10f}\")\n",
    "print(f\"Momentum : {len(traj_momentum_g)} itérations, f(x*) = {costs_momentum_g[-1]:.10f}\")\n",
    "print(f\"Nesterov : {len(traj_nesterov_g)} itérations, f(x*) = {costs_nesterov_g[-1]:.10f}\")\n",
    "print(f\"Adam : {len(traj_adam_g)} itérations, f(x*) = {costs_adam_g[-1]:.10f}\")\n",
    "\n",
    "# Graphe de comparaison pour g\n",
    "trajectories_g = {\n",
    "    'Simple': traj_simple_g,\n",
    "    'Momentum': traj_momentum_g,\n",
    "    'Nesterov': traj_nesterov_g,\n",
    "    'Adam': traj_adam_g\n",
    "}\n",
    "\n",
    "fig, _ = plot_comparison_trajectories(\n",
    "    g, trajectories_g,\n",
    "    x_range=(-0.5, 0.5), y_range=(-0.5, 0.5),\n",
    "    title=\"Fonction g : Comparaison des Algorithmes\"\n",
    ")\n",
    "plt.savefig('../figures/temp/g_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Courbes de convergence pour g\n",
    "costs_g = {\n",
    "    'Simple': costs_simple_g,\n",
    "    'Momentum': costs_momentum_g,\n",
    "    'Nesterov': costs_nesterov_g,\n",
    "    'Adam': costs_adam_g\n",
    "}\n",
    "\n",
    "fig, _ = plot_convergence_curves(\n",
    "    costs_g, title=\"Fonction g : Convergence\"\n",
    ")\n",
    "plt.savefig('../figures/temp/g_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# TEST 2 : Fonction h (polynôme complexe)\n",
    "# =========================================================================\n",
    "\n",
    "print(\"\\n--- Fonction h : x²y - 2xy³ + 3xy + 4 ---\")\n",
    "x0 = np.array([0.5, 0.5])  # Point de départ quelconque\n",
    "                            # MODIF : point plus proche -> 0.5 au lieu de 2.0, 1.0\n",
    "\n",
    "traj_simple_h, costs_simple_h = gradient_descent(\n",
    "    h, grad_h, x0, learning_rate=0.01, max_iter=500, tol=1e-6  # ← 0.01 au lieu de 0.0005\n",
    ")\n",
    "\n",
    "traj_momentum_h, costs_momentum_h = gradient_descent_momentum(\n",
    "    h, grad_h, x0, learning_rate=0.01, momentum=0.8, max_iter=500, tol=1e-6  # ← 0.01 et 0.8\n",
    ")\n",
    "\n",
    "traj_nesterov_h, costs_nesterov_h = gradient_descent_nesterov(\n",
    "    h, grad_h, x0, learning_rate=0.01, momentum=0.8, max_iter=500, tol=1e-6  # ← 0.01 et 0.8\n",
    ")\n",
    "\n",
    "traj_adam_h, costs_adam_h = gradient_descent_adam(\n",
    "    h, grad_h, x0, learning_rate=0.001, max_iter=500, tol=1e-6  # ← 0.001 pour éviter divergence\n",
    ")\n",
    "\n",
    "print(f\"\\nSimple : {len(traj_simple_h)} itérations, f(x*) = {costs_simple_h[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_simple_h[-1][0]:.4f}, {traj_simple_h[-1][1]:.4f})\")\n",
    "\n",
    "print(f\"\\nMomentum : {len(traj_momentum_h)} itérations, f(x*) = {costs_momentum_h[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_momentum_h[-1][0]:.4f}, {traj_momentum_h[-1][1]:.4f})\")\n",
    "\n",
    "print(f\"\\nNesterov : {len(traj_nesterov_h)} itérations, f(x*) = {costs_nesterov_h[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_nesterov_h[-1][0]:.4f}, {traj_nesterov_h[-1][1]:.4f})\")\n",
    "\n",
    "print(f\"\\nAdam : {len(traj_adam_h)} itérations, f(x*) = {costs_adam_h[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_adam_h[-1][0]:.4f}, {traj_adam_h[-1][1]:.4f})\")\n",
    "\n",
    "# Graphe de comparaison pour h\n",
    "trajectories_h = {\n",
    "    'Simple': traj_simple_h,\n",
    "    'Momentum': traj_momentum_h,\n",
    "    'Nesterov': traj_nesterov_h,\n",
    "    'Adam': traj_adam_h\n",
    "}\n",
    "\n",
    "fig, _ = plot_comparison_trajectories(\n",
    "    h, trajectories_h,\n",
    "    x_range=(-2, 3), y_range=(-2, 2),\n",
    "    title=\"Fonction h : Comparaison des Algorithmes\"\n",
    ")\n",
    "plt.savefig('../figures/temp/h_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Courbes de convergence pour h\n",
    "costs_h = {\n",
    "    'Simple': costs_simple_h,\n",
    "    'Momentum': costs_momentum_h,\n",
    "    'Nesterov': costs_nesterov_h,\n",
    "    'Adam': costs_adam_h\n",
    "}\n",
    "\n",
    "fig, _ = plot_convergence_curves(\n",
    "    costs_h, title=\"Fonction h : Convergence\"\n",
    ")\n",
    "plt.savefig('../figures/temp/h_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Fonction de Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPÉRIENCE 3 : Fonction de Rosenbrock\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Point de départ\n",
    "x0 = np.array([-1.0, 1.0])\n",
    "\n",
    "# Exécution des algorithmes\n",
    "traj_simple, costs_simple = gradient_descent(\n",
    "    rosenbrock, grad_rosenbrock, x0, learning_rate=0.001, max_iter=2000\n",
    ")\n",
    "\n",
    "traj_momentum, costs_momentum = gradient_descent_momentum(\n",
    "    rosenbrock, grad_rosenbrock, x0, learning_rate=0.001, momentum=0.9, max_iter=2000\n",
    ")\n",
    "\n",
    "traj_nesterov, costs_nesterov = gradient_descent_nesterov(\n",
    "    rosenbrock, grad_rosenbrock, x0, learning_rate=0.001, momentum=0.9, max_iter=2000\n",
    ")\n",
    "\n",
    "traj_adam, costs_adam = gradient_descent_adam(\n",
    "    rosenbrock, grad_rosenbrock, x0, learning_rate=0.01, max_iter=2000\n",
    ")\n",
    "\n",
    "# Affichage des résultats\n",
    "print(f\"\\nSimple : {len(traj_simple)} itérations, f(x*) = {costs_simple[-1]:.10f}\")\n",
    "print(f\"Momentum : {len(traj_momentum)} itérations, f(x*) = {costs_momentum[-1]:.10f}\")\n",
    "print(f\"Nesterov : {len(traj_nesterov)} itérations, f(x*) = {costs_nesterov[-1]:.10f}\")\n",
    "print(f\"Adam : {len(traj_adam)} itérations, f(x*) = {costs_adam[-1]:.10f}\")\n",
    "\n",
    "# Graphe individuel : Adam (le plus performant)\n",
    "fig1, _ = plot_trajectory_2d(\n",
    "    rosenbrock, traj_adam, costs_adam,\n",
    "    x_range=(-2, 2), y_range=(-1, 3),\n",
    "    title=\"Rosenbrock : Adam\",\n",
    "    algo_name=\"Adam\"\n",
    ")\n",
    "plt.savefig('../figures/temp/rosenbrock_adam.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Graphe de comparaison\n",
    "trajectories = {\n",
    "    'Simple': traj_simple,\n",
    "    'Momentum': traj_momentum,\n",
    "    'Nesterov': traj_nesterov,\n",
    "    'Adam': traj_adam\n",
    "}\n",
    "\n",
    "fig2, _ = plot_comparison_trajectories(\n",
    "    rosenbrock, trajectories,\n",
    "    x_range=(-2, 2), y_range=(-1, 3),\n",
    "    title=\"Rosenbrock : Comparaison des Algorithmes\"\n",
    ")\n",
    "plt.savefig('../figures/temp/rosenbrock_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Courbes de convergence\n",
    "costs = {\n",
    "    'Simple': costs_simple,\n",
    "    'Momentum': costs_momentum,\n",
    "    'Nesterov': costs_nesterov,\n",
    "    'Adam': costs_adam\n",
    "}\n",
    "\n",
    "fig3, _ = plot_convergence_curves(\n",
    "    costs, title=\"Rosenbrock : Convergence\"\n",
    ")\n",
    "plt.savefig('../figures/temp/rosenbrock_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Fonction de Booth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPÉRIENCE 4 : Fonction de Booth\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Point de départ\n",
    "x0 = np.array([0.0, 0.0])\n",
    "\n",
    "# Exécution des algorithmes\n",
    "traj_simple, costs_simple = gradient_descent(\n",
    "    booth, grad_booth, x0, learning_rate=0.01\n",
    ")\n",
    "\n",
    "traj_momentum, costs_momentum = gradient_descent_momentum(\n",
    "    booth, grad_booth, x0, learning_rate=0.01, momentum=0.9\n",
    ")\n",
    "\n",
    "traj_nesterov, costs_nesterov = gradient_descent_nesterov(\n",
    "    booth, grad_booth, x0, learning_rate=0.01, momentum=0.9\n",
    ")\n",
    "\n",
    "traj_adam, costs_adam = gradient_descent_adam(\n",
    "    booth, grad_booth, x0, learning_rate=0.1\n",
    ")\n",
    "\n",
    "# Affichage des résultats\n",
    "print(f\"\\nSimple : {len(traj_simple)} itérations, f(x*) = {costs_simple[-1]:.10f}\")\n",
    "print(f\"Momentum : {len(traj_momentum)} itérations, f(x*) = {costs_momentum[-1]:.10f}\")\n",
    "print(f\"Nesterov : {len(traj_nesterov)} itérations, f(x*) = {costs_nesterov[-1]:.10f}\")\n",
    "print(f\"Adam : {len(traj_adam)} itérations, f(x*) = {costs_adam[-1]:.10f}\")\n",
    "\n",
    "# Graphe de comparaison\n",
    "trajectories = {\n",
    "    'Simple': traj_simple,\n",
    "    'Momentum': traj_momentum,\n",
    "    'Nesterov': traj_nesterov,\n",
    "    'Adam': traj_adam\n",
    "}\n",
    "\n",
    "fig, _ = plot_comparison_trajectories(\n",
    "    booth, trajectories,\n",
    "    x_range=(-2, 4), y_range=(-1, 5),\n",
    "    title=\"Booth : Comparaison des Algorithmes\"\n",
    ")\n",
    "plt.savefig('../figures/temp/booth_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Courbes de convergence\n",
    "costs = {\n",
    "    'Simple': costs_simple,\n",
    "    'Momentum': costs_momentum,\n",
    "    'Nesterov': costs_nesterov,\n",
    "    'Adam': costs_adam\n",
    "}\n",
    "\n",
    "fig, _ = plot_convergence_curves(\n",
    "    costs, title=\"Booth : Convergence\"\n",
    ")\n",
    "plt.savefig('../figures/temp/booth_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Fonction de Beale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPÉRIENCE 5 : Fonction de Beale\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Point de départ\n",
    "x0 = np.array([0.0, 0.0])\n",
    "\n",
    "# Exécution des algorithmes\n",
    "traj_simple, costs_simple = gradient_descent(\n",
    "    beale, grad_beale, x0, learning_rate=0.001, max_iter=2000\n",
    ")\n",
    "\n",
    "traj_momentum, costs_momentum = gradient_descent_momentum(\n",
    "    beale, grad_beale, x0, learning_rate=0.001, momentum=0.9, max_iter=2000\n",
    ")\n",
    "\n",
    "traj_nesterov, costs_nesterov = gradient_descent_nesterov(\n",
    "    beale, grad_beale, x0, learning_rate=0.001, momentum=0.9, max_iter=2000\n",
    ")\n",
    "\n",
    "traj_adam, costs_adam = gradient_descent_adam(\n",
    "    beale, grad_beale, x0, learning_rate=0.01, max_iter=2000\n",
    ")\n",
    "\n",
    "# Affichage des résultats\n",
    "print(f\"\\nSimple : {len(traj_simple)} itérations, f(x*) = {costs_simple[-1]:.10f}\")\n",
    "print(f\"Momentum : {len(traj_momentum)} itérations, f(x*) = {costs_momentum[-1]:.10f}\")\n",
    "print(f\"Nesterov : {len(traj_nesterov)} itérations, f(x*) = {costs_nesterov[-1]:.10f}\")\n",
    "print(f\"Adam : {len(traj_adam)} itérations, f(x*) = {costs_adam[-1]:.10f}\")\n",
    "\n",
    "# Graphe de comparaison\n",
    "trajectories = {\n",
    "    'Simple': traj_simple,\n",
    "    'Momentum': traj_momentum,\n",
    "    'Nesterov': traj_nesterov,\n",
    "    'Adam': traj_adam\n",
    "}\n",
    "\n",
    "fig, _ = plot_comparison_trajectories(\n",
    "    beale, trajectories,\n",
    "    x_range=(-1, 4), y_range=(-1, 2),\n",
    "    title=\"Beale : Comparaison des Algorithmes\"\n",
    ")\n",
    "plt.savefig('../figures/temp/beale_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Courbes de convergence\n",
    "costs = {\n",
    "    'Simple': costs_simple,\n",
    "    'Momentum': costs_momentum,\n",
    "    'Nesterov': costs_nesterov,\n",
    "    'Adam': costs_adam\n",
    "}\n",
    "\n",
    "fig, _ = plot_convergence_curves(\n",
    "    costs, title=\"Beale : Convergence\"\n",
    ")\n",
    "plt.savefig('../figures/temp/beale_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Fonction de Himmelblau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPÉRIENCE 6 : Fonction de Himmelblau\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Point de départ (plusieurs essais possibles pour trouver différents minima)\n",
    "x0 = np.array([0.0, 0.0])\n",
    "\n",
    "# Exécution des algorithmes\n",
    "traj_simple, costs_simple = gradient_descent(\n",
    "    himmelblau, grad_himmelblau, x0, learning_rate=0.01\n",
    ")\n",
    "\n",
    "traj_momentum, costs_momentum = gradient_descent_momentum(\n",
    "    himmelblau, grad_himmelblau, x0, learning_rate=0.01, momentum=0.7\n",
    ")\n",
    "\n",
    "traj_nesterov, costs_nesterov = gradient_descent_nesterov(\n",
    "    himmelblau, grad_himmelblau, x0, learning_rate=0.01, momentum=0.7\n",
    ")\n",
    "\n",
    "traj_adam, costs_adam = gradient_descent_adam(\n",
    "    himmelblau, grad_himmelblau, x0, learning_rate=0.1\n",
    ")\n",
    "\n",
    "# Affichage des résultats\n",
    "print(f\"\\nSimple : {len(traj_simple)} itérations, f(x*) = {costs_simple[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_simple[-1][0]:.4f}, {traj_simple[-1][1]:.4f})\")\n",
    "\n",
    "print(f\"\\nMomentum : {len(traj_momentum)} itérations, f(x*) = {costs_momentum[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_momentum[-1][0]:.4f}, {traj_momentum[-1][1]:.4f})\")\n",
    "\n",
    "print(f\"\\nNesterov : {len(traj_nesterov)} itérations, f(x*) = {costs_nesterov[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_nesterov[-1][0]:.4f}, {traj_nesterov[-1][1]:.4f})\")\n",
    "\n",
    "print(f\"\\nAdam : {len(traj_adam)} itérations, f(x*) = {costs_adam[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_adam[-1][0]:.4f}, {traj_adam[-1][1]:.4f})\")\n",
    "\n",
    "# Graphe de comparaison\n",
    "trajectories = {\n",
    "    'Simple': traj_simple,\n",
    "    'Momentum': traj_momentum,\n",
    "    'Nesterov': traj_nesterov,\n",
    "    'Adam': traj_adam\n",
    "}\n",
    "\n",
    "fig, _ = plot_comparison_trajectories(\n",
    "    himmelblau, trajectories,\n",
    "    x_range=(-5, 5), y_range=(-5, 5),\n",
    "    title=\"Himmelblau : Comparaison des Algorithmes\"\n",
    ")\n",
    "plt.savefig('../figures/temp/himmelblau_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Courbes de convergence\n",
    "costs = {\n",
    "    'Simple': costs_simple,\n",
    "    'Momentum': costs_momentum,\n",
    "    'Nesterov': costs_nesterov,\n",
    "    'Adam': costs_adam\n",
    "}\n",
    "\n",
    "fig, _ = plot_convergence_curves(\n",
    "    costs, title=\"Himmelblau : Convergence\"\n",
    ")\n",
    "plt.savefig('../figures/temp/himmelblau_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Fonction d'Ackley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPÉRIENCE 7 : Fonction d'Ackley\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Définition de la fonction d'Ackley\n",
    "def ackley(x):\n",
    "    \"\"\"\n",
    "    Fonction d'Ackley : très difficile avec centaines de minima locaux.\n",
    "    Minimum global : (0, 0) avec f(0,0) = 0\n",
    "    \"\"\"\n",
    "    a = 20\n",
    "    b = 0.2\n",
    "    c = 2 * np.pi\n",
    "    d = 2  # dimension\n",
    "    \n",
    "    sum_sq = x[0]**2 + x[1]**2\n",
    "    sum_cos = np.cos(c*x[0]) + np.cos(c*x[1])\n",
    "    \n",
    "    term1 = -a * np.exp(-b * np.sqrt(sum_sq / d))\n",
    "    term2 = -np.exp(sum_cos / d)\n",
    "    \n",
    "    return term1 + term2 + a + np.e\n",
    "\n",
    "def grad_ackley(x):\n",
    "    \"\"\"Gradient numérique d'Ackley (analytique complexe)\"\"\"\n",
    "    h = 1e-5\n",
    "    gx = (ackley(x + np.array([h, 0])) - ackley(x - np.array([h, 0]))) / (2*h)\n",
    "    gy = (ackley(x + np.array([0, h])) - ackley(x - np.array([0, h]))) / (2*h)\n",
    "    return np.array([gx, gy])\n",
    "\n",
    "# Point de départ : pas trop loin pour avoir une chance de trouver le global\n",
    "x0 = np.array([2.0, 2.0])\n",
    "\n",
    "# Exécution des algorithmes\n",
    "traj_simple, costs_simple = gradient_descent(\n",
    "    ackley, grad_ackley, x0, learning_rate=0.01, max_iter=2000\n",
    ")\n",
    "\n",
    "traj_momentum, costs_momentum = gradient_descent_momentum(\n",
    "    ackley, grad_ackley, x0, learning_rate=0.01, momentum=0.9, max_iter=2000\n",
    ")\n",
    "\n",
    "traj_nesterov, costs_nesterov = gradient_descent_nesterov(\n",
    "    ackley, grad_ackley, x0, learning_rate=0.01, momentum=0.9, max_iter=2000\n",
    ")\n",
    "\n",
    "traj_adam, costs_adam = gradient_descent_adam(\n",
    "    ackley, grad_ackley, x0, learning_rate=0.1, max_iter=2000\n",
    ")\n",
    "\n",
    "# Affichage des résultats\n",
    "print(f\"\\nSimple : {len(traj_simple)} itérations, f(x*) = {costs_simple[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_simple[-1][0]:.4f}, {traj_simple[-1][1]:.4f})\")\n",
    "\n",
    "print(f\"\\nMomentum : {len(traj_momentum)} itérations, f(x*) = {costs_momentum[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_momentum[-1][0]:.4f}, {traj_momentum[-1][1]:.4f})\")\n",
    "\n",
    "print(f\"\\nNesterov : {len(traj_nesterov)} itérations, f(x*) = {costs_nesterov[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_nesterov[-1][0]:.4f}, {traj_nesterov[-1][1]:.4f})\")\n",
    "\n",
    "print(f\"\\nAdam : {len(traj_adam)} itérations, f(x*) = {costs_adam[-1]:.10f}\")\n",
    "print(f\"Point final : ({traj_adam[-1][0]:.4f}, {traj_adam[-1][1]:.4f})\")\n",
    "\n",
    "# Vérification : qui a trouvé le minimum global (0,0) ?\n",
    "tolerance = 0.1\n",
    "for name, traj in [('Simple', traj_simple), ('Momentum', traj_momentum), \n",
    "                    ('Nesterov', traj_nesterov), ('Adam', traj_adam)]:\n",
    "    dist = np.linalg.norm(traj[-1])\n",
    "    if dist < tolerance:\n",
    "        print(f\"✅ {name} a trouvé le minimum global ! (distance = {dist:.4f})\")\n",
    "    else:\n",
    "        print(f\"❌ {name} est bloqué dans un minimum local (distance au global = {dist:.4f})\")\n",
    "\n",
    "# Graphe individuel : Adam (espérons qu'il trouve le global)\n",
    "fig1, _ = plot_trajectory_2d(\n",
    "    ackley, traj_adam, costs_adam,\n",
    "    x_range=(-3, 3), y_range=(-3, 3),\n",
    "    title=\"Ackley : Adam\",\n",
    "    algo_name=\"Adam\",\n",
    "    num_levels=50  # Plus de niveaux pour voir les oscillations\n",
    ")\n",
    "plt.savefig('../figures/temp/ackley_adam.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Graphe de comparaison\n",
    "trajectories = {\n",
    "    'Simple': traj_simple,\n",
    "    'Momentum': traj_momentum,\n",
    "    'Nesterov': traj_nesterov,\n",
    "    'Adam': traj_adam\n",
    "}\n",
    "\n",
    "fig2, _ = plot_comparison_trajectories(\n",
    "    ackley, trajectories,\n",
    "    x_range=(-3, 3), y_range=(-3, 3),\n",
    "    title=\"Ackley : Comparaison des Algorithmes\",\n",
    "    num_levels=50\n",
    ")\n",
    "plt.savefig('../figures/temp/ackley_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Courbes de convergence\n",
    "costs = {\n",
    "    'Simple': costs_simple,\n",
    "    'Momentum': costs_momentum,\n",
    "    'Nesterov': costs_nesterov,\n",
    "    'Adam': costs_adam\n",
    "}\n",
    "\n",
    "fig3, _ = plot_convergence_curves(\n",
    "    costs, title=\"Ackley : Convergence\"\n",
    ")\n",
    "plt.savefig('../figures/temp/ackley_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.8 Comparaison - Dual Numbers vs Dérivée Numérique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPÉRIENCE 8 : Dual Numbers vs Dérivée Numérique\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# =========================================================================\n",
    "# IMPLÉMENTATION DES DUAL NUMBERS\n",
    "# =========================================================================\n",
    "\n",
    "class Dual:\n",
    "    \"\"\"\n",
    "    Nombre dual de la forme a + b·ε avec ε² = 0\n",
    "    Utilisé pour la dérivation automatique.\n",
    "    \"\"\"\n",
    "    def __init__(self, real, dual=0.0):\n",
    "        self.real = float(real)\n",
    "        self.dual = float(dual)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Dual({self.real}, {self.dual})\"\n",
    "    \n",
    "        # ========== AJOUT DE MÉTHODES ==========\n",
    "    \n",
    "    def __neg__(self):\n",
    "        \"\"\"Négation : -Dual\"\"\"\n",
    "        return Dual(-self.real, -self.dual)\n",
    "    \n",
    "    def __abs__(self):\n",
    "        \"\"\"Valeur absolue (pour comparaisons)\"\"\"\n",
    "        return abs(self.real)\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        \"\"\"Comparaison < (pour sqrt, etc.)\"\"\"\n",
    "        if isinstance(other, Dual):\n",
    "            return self.real < other.real\n",
    "        return self.real < other\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        \"\"\"Comparaison >\"\"\"\n",
    "        if isinstance(other, Dual):\n",
    "            return self.real > other.real\n",
    "        return self.real > other\n",
    "    \n",
    "    def __le__(self, other):\n",
    "        \"\"\"Comparaison <=\"\"\"\n",
    "        if isinstance(other, Dual):\n",
    "            return self.real <= other.real\n",
    "        return self.real <= other\n",
    "    \n",
    "    def __ge__(self, other):\n",
    "        \"\"\"Comparaison >=\"\"\"\n",
    "        if isinstance(other, Dual):\n",
    "            return self.real >= other.real\n",
    "        return self.real >= other\n",
    "    \n",
    "    # ========== FIN DES AJOUTS ==========\n",
    "    \n",
    "    # Addition\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Dual):\n",
    "            return Dual(self.real + other.real, self.dual + other.dual)\n",
    "        return Dual(self.real + other, self.dual)\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self.__add__(other)\n",
    "    \n",
    "    # Soustraction\n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, Dual):\n",
    "            return Dual(self.real - other.real, self.dual - other.dual)\n",
    "        return Dual(self.real - other, self.dual)\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        return Dual(other - self.real, -self.dual)\n",
    "    \n",
    "    # Multiplication\n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, Dual):\n",
    "            # (a+bε)(c+dε) = ac + (ad+bc)ε\n",
    "            return Dual(\n",
    "                self.real * other.real,\n",
    "                self.real * other.dual + self.dual * other.real\n",
    "            )\n",
    "        return Dual(self.real * other, self.dual * other)\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self.__mul__(other)\n",
    "    \n",
    "    # Division\n",
    "    def __truediv__(self, other):\n",
    "        if isinstance(other, Dual):\n",
    "            # (a+bε)/(c+dε) = a/c + (bc-ad)/c²·ε\n",
    "            return Dual(\n",
    "                self.real / other.real,\n",
    "                (self.dual * other.real - self.real * other.dual) / (other.real ** 2)\n",
    "            )\n",
    "        return Dual(self.real / other, self.dual / other)\n",
    "    \n",
    "    # Puissance\n",
    "    def __pow__(self, n):\n",
    "        # (a+bε)^n = a^n + n·a^(n-1)·b·ε\n",
    "        return Dual(\n",
    "            self.real ** n,\n",
    "            n * (self.real ** (n-1)) * self.dual\n",
    "        )\n",
    "    \n",
    "    # Valeur absolue et comparaisons (pour sqrt)\n",
    "    def __abs__(self):\n",
    "        return abs(self.real)\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        if isinstance(other, Dual):\n",
    "            return self.real < other.real\n",
    "        return self.real < other\n",
    "\n",
    "\n",
    "# Fonctions mathématiques pour Dual\n",
    "def dual_exp(x):\n",
    "    \"\"\"exp(a+bε) = exp(a) + b·exp(a)·ε\"\"\"\n",
    "    if isinstance(x, Dual):\n",
    "        exp_real = np.exp(x.real)\n",
    "        return Dual(exp_real, x.dual * exp_real)\n",
    "    return np.exp(x)\n",
    "\n",
    "def dual_sqrt(x):\n",
    "    \"\"\"sqrt(a+bε) = sqrt(a) + b/(2√a)·ε\"\"\"\n",
    "    if isinstance(x, Dual):\n",
    "        sqrt_real = np.sqrt(x.real)\n",
    "        return Dual(sqrt_real, x.dual / (2 * sqrt_real))\n",
    "    return np.sqrt(x)\n",
    "\n",
    "def dual_cos(x):\n",
    "    \"\"\"cos(a+bε) = cos(a) - b·sin(a)·ε\"\"\"\n",
    "    if isinstance(x, Dual):\n",
    "        return Dual(np.cos(x.real), -x.dual * np.sin(x.real))\n",
    "    return np.cos(x)\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# ACKLEY AVEC DUAL NUMBERS\n",
    "# =========================================================================\n",
    "\n",
    "def ackley_dual(x, y):\n",
    "    \"\"\"\n",
    "    Version d'Ackley qui accepte des Dual numbers.\n",
    "    \"\"\"\n",
    "    a = 20\n",
    "    b = 0.2\n",
    "    c = 2 * np.pi\n",
    "    d = 2\n",
    "    \n",
    "    # Calculs avec Dual\n",
    "    sum_sq = x*x + y*y\n",
    "    sqrt_term = dual_sqrt(sum_sq / d)\n",
    "    term1 = -a * dual_exp(-b * sqrt_term)\n",
    "    \n",
    "    sum_cos = dual_cos(c*x) + dual_cos(c*y)\n",
    "    term2 = -dual_exp(sum_cos / d)\n",
    "    \n",
    "    result = term1 + term2 + a + np.e\n",
    "    return result\n",
    "\n",
    "\n",
    "def gradient_dual_ackley(x_point):\n",
    "    \"\"\"\n",
    "    Calcule le gradient d'Ackley avec dual numbers.\n",
    "    \"\"\"\n",
    "    x_val, y_val = x_point[0], x_point[1]\n",
    "    \n",
    "    # Gradient selon x : mettre ε sur x\n",
    "    x_dual = Dual(x_val, 1.0)  # x + ε\n",
    "    y_dual = Dual(y_val, 0.0)  # y + 0·ε\n",
    "    result_x = ackley_dual(x_dual, y_dual)\n",
    "    gx = result_x.dual\n",
    "    \n",
    "    # Gradient selon y : mettre ε sur y\n",
    "    x_dual = Dual(x_val, 0.0)  # x + 0·ε\n",
    "    y_dual = Dual(y_val, 1.0)  # y + ε\n",
    "    result_y = ackley_dual(x_dual, y_dual)\n",
    "    gy = result_y.dual\n",
    "    \n",
    "    return np.array([gx, gy])\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# COMPARAISON PRÉCISION\n",
    "# =========================================================================\n",
    "\n",
    "print(\"\\n--- Test de Précision ---\")\n",
    "print(\"Point de test : (1.5, 2.3)\")\n",
    "\n",
    "x_test = np.array([1.5, 2.3])\n",
    "\n",
    "# Gradient numérique\n",
    "grad_num = grad_ackley(x_test)\n",
    "\n",
    "# Gradient avec dual numbers\n",
    "grad_dual = gradient_dual_ackley(x_test)\n",
    "\n",
    "print(f\"\\nGradient numérique : [{grad_num[0]:.10f}, {grad_num[1]:.10f}]\")\n",
    "print(f\"Gradient dual      : [{grad_dual[0]:.10f}, {grad_dual[1]:.10f}]\")\n",
    "\n",
    "# Différence relative\n",
    "diff = np.abs(grad_dual - grad_num)\n",
    "rel_error = np.linalg.norm(diff) / np.linalg.norm(grad_dual) * 100\n",
    "\n",
    "print(f\"\\nDifférence absolue : [{diff[0]:.2e}, {diff[1]:.2e}]\")\n",
    "print(f\"Erreur relative    : {rel_error:.6f} %\")\n",
    "\n",
    "if rel_error < 0.01:\n",
    "    print(\"✅ Excellente concordance (< 0.01%)\")\n",
    "elif rel_error < 0.1:\n",
    "    print(\"✅ Bonne concordance (< 0.1%)\")\n",
    "elif rel_error < 1.0:\n",
    "    print(\"⚠️  Concordance acceptable (< 1%)\")\n",
    "else:\n",
    "    print(\"❌ Différence significative (> 1%)\")\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# COMPARAISON TEMPS DE CALCUL\n",
    "# =========================================================================\n",
    "\n",
    "print(\"\\n--- Test de Performance ---\")\n",
    "\n",
    "import time\n",
    "\n",
    "n_iterations = 1000\n",
    "print(f\"Nombre de tests : {n_iterations}\")\n",
    "\n",
    "# Test gradient numérique\n",
    "start = time.time()\n",
    "for _ in range(n_iterations):\n",
    "    _ = grad_ackley(x_test)\n",
    "time_num = time.time() - start\n",
    "\n",
    "# Test gradient dual\n",
    "start = time.time()\n",
    "for _ in range(n_iterations):\n",
    "    _ = gradient_dual_ackley(x_test)\n",
    "time_dual = time.time() - start\n",
    "\n",
    "print(f\"\\nTemps numérique : {time_num*1000:.2f} ms\")\n",
    "print(f\"Temps dual      : {time_dual*1000:.2f} ms\")\n",
    "print(f\"Ratio           : {time_dual/time_num:.2f}x\")\n",
    "\n",
    "if time_dual < time_num:\n",
    "    print(f\"✅ Dual numbers {time_num/time_dual:.2f}x plus rapide\")\n",
    "elif time_dual < time_num * 1.5:\n",
    "    print(\"➡️  Performances comparables\")\n",
    "else:\n",
    "    print(f\"⚠️  Numérique {time_dual/time_num:.2f}x plus rapide\")\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# TEST SUR PLUSIEURS POINTS\n",
    "# =========================================================================\n",
    "\n",
    "print(\"\\n--- Test sur Échantillon de Points ---\")\n",
    "\n",
    "test_points = [\n",
    "    np.array([0.0, 0.0]),\n",
    "    np.array([1.0, 1.0]),\n",
    "    np.array([2.0, 2.0]),\n",
    "    np.array([-1.5, 0.5]),\n",
    "    np.array([0.3, -0.8])\n",
    "]\n",
    "\n",
    "print(\"\\n| Point          | Erreur Rel. (%) | Concordance |\")\n",
    "print(\"|----------------|-----------------|-------------|\")\n",
    "\n",
    "for pt in test_points:\n",
    "    grad_n = grad_ackley(pt)\n",
    "    grad_d = gradient_dual_ackley(pt)\n",
    "    \n",
    "    diff = np.linalg.norm(grad_d - grad_n)\n",
    "    rel = diff / np.linalg.norm(grad_d) * 100\n",
    "    \n",
    "    status = \"✅\" if rel < 0.1 else \"⚠️\" if rel < 1.0 else \"❌\"\n",
    "    print(f\"| ({pt[0]:5.1f}, {pt[1]:5.1f}) | {rel:14.6f}  | {status:11s} |\")\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# VISUALISATION COMPARATIVE\n",
    "# =========================================================================\n",
    "\n",
    "print(\"\\n--- Génération de la figure comparative ---\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Zone de test\n",
    "x_range = np.linspace(-3, 3, 30)\n",
    "y_range = np.linspace(-3, 3, 30)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "# Calculer les normes des gradients\n",
    "grad_norm_num = np.zeros_like(X)\n",
    "grad_norm_dual = np.zeros_like(X)\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        pt = np.array([X[i,j], Y[i,j]])\n",
    "        \n",
    "        gn = grad_ackley(pt)\n",
    "        gd = gradient_dual_ackley(pt)\n",
    "        \n",
    "        grad_norm_num[i,j] = np.linalg.norm(gn)\n",
    "        grad_norm_dual[i,j] = np.linalg.norm(gd)\n",
    "\n",
    "# Graphe 1 : Gradient numérique\n",
    "im1 = axes[0].contourf(X, Y, grad_norm_num, levels=20, cmap='viridis')\n",
    "axes[0].set_title('Norme du Gradient - Dérivée Numérique', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('y')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Graphe 2 : Gradient dual\n",
    "im2 = axes[1].contourf(X, Y, grad_norm_dual, levels=20, cmap='viridis')\n",
    "axes[1].set_title('Norme du Gradient - Dual Numbers', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('y')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/temp/ackley_gradient_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "Les deux méthodes donnent des résultats très proches (erreur < 0.1%).\n",
    "\n",
    "DUAL NUMBERS :\n",
    "  ✅ Gradient EXACT (pas d'approximation)\n",
    "  ✅ Pas de paramètre h à ajuster\n",
    "  ✅ Bonne précision numérique\n",
    "  ❌ Implémentation plus complexe\n",
    "  \n",
    "DÉRIVÉE NUMÉRIQUE :\n",
    "  ✅ Très simple à implémenter\n",
    "  ✅ Marche pour toute fonction\n",
    "  ❌ Approximation (dépend de h)\n",
    "  ❌ Sensible aux erreurs d'arrondi\n",
    "  \n",
    "Pour Ackley et la plupart des fonctions, les deux méthodes sont valides.\n",
    "En pratique : Dual numbers préférable pour précision, numérique pour simplicité.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tableau récapitulatif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TABLEAU RÉCAPITULATIF DES PERFORMANCES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Créer un tableau comparatif\n",
    "data = {\n",
    "    'Fonction': ['Quadratique', 'Rosenbrock', 'Booth', 'Beale', 'Himmelblau'],\n",
    "    'Simple': ['74 itér.', '2000+ itér.', 'XX itér.', 'XX itér.', 'XX itér.'],\n",
    "    'Momentum': ['101 itér.', '2000+ itér.', 'XX itér.', 'XX itér.', 'XX itér.'],\n",
    "    'Nesterov': ['92 itér.', '2000+ itér.', 'XX itér.', 'XX itér.', 'XX itér.'],\n",
    "    'Adam': ['XX itér.', 'XX itér.', 'XX itér.', 'XX itér.', 'XX itér.']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\nNote : Remplacer les XX par les vraies valeurs après exécution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Cas d'échec et diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CATALOGUE DES CAS D'ÉCHEC\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nObjectif : Comprendre pourquoi et comment les algorithmes échouent\")\n",
    "print(\"Importance : Justifier les améliorations (Momentum, Adam, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Learning Rate trop grand : Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"CAS D'ÉCHEC 1 : Learning Rate Trop Grand → Divergence\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# On reprend la fonction quadratique simple\n",
    "x0 = np.array([2.0, 2.0])\n",
    "\n",
    "# Test avec plusieurs learning rates\n",
    "learning_rates = [0.1, 0.5, 1.0, 1.5]\n",
    "trajectories_fail1 = {}\n",
    "costs_fail1 = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    try:\n",
    "        traj, costs = gradient_descent(\n",
    "            quadratique, grad_quadratique, x0, \n",
    "            learning_rate=lr, max_iter=50\n",
    "        )\n",
    "        trajectories_fail1[f'α={lr}'] = traj\n",
    "        costs_fail1[f'α={lr}'] = costs\n",
    "        \n",
    "        # Diagnostic\n",
    "        final_cost = costs[-1]\n",
    "        if final_cost > costs[0]:\n",
    "            print(f\"❌ α={lr} : DIVERGENCE ! Coût passe de {costs[0]:.2f} à {final_cost:.2e}\")\n",
    "        elif np.isnan(final_cost) or np.isinf(final_cost):\n",
    "            print(f\"❌ α={lr} : EXPLOSION ! NaN ou Inf atteint\")\n",
    "        elif final_cost < 1e-6:\n",
    "            print(f\"✅ α={lr} : Converge normalement vers {final_cost:.2e}\")\n",
    "        else:\n",
    "            print(f\"⚠️  α={lr} : Converge mais instable, coût final = {final_cost:.2e}\")\n",
    "    except:\n",
    "        print(f\"❌ α={lr} : CRASH ! Overflow\")\n",
    "        # Créer trajectoire vide pour le graphe\n",
    "        trajectories_fail1[f'α={lr}'] = np.array([[np.nan, np.nan]])\n",
    "        costs_fail1[f'α={lr}'] = np.array([np.nan])\n",
    "\n",
    "# Graphe de comparaison\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "# Courbes de niveau\n",
    "x = np.linspace(-5, 5, 200)\n",
    "y = np.linspace(-5, 5, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = quadratique(np.array([X[i, j], Y[i, j]]))\n",
    "\n",
    "ax.contour(X, Y, Z, levels=30, cmap='viridis', linewidths=0.8)\n",
    "ax.contourf(X, Y, Z, levels=30, cmap='viridis', alpha=0.3)\n",
    "\n",
    "# Trajectoires avec couleurs différentes\n",
    "colors = ['#6A994E', '#E9A319', '#E07A5F', 'darkred']\n",
    "for (name, traj), color in zip(trajectories_fail1.items(), colors):\n",
    "    if len(traj) > 1 and not np.isnan(traj[0][0]):\n",
    "        # Limiter l'affichage aux 10 premiers points si divergence\n",
    "        display_traj = traj[:min(10, len(traj))]\n",
    "        ax.plot(display_traj[:, 0], display_traj[:, 1], \n",
    "               color=color, linewidth=2, marker='o', markersize=4,\n",
    "               label=f'{name} ({len(traj)} itér.)')\n",
    "        \n",
    "        # Marquer le début et la fin\n",
    "        ax.plot(traj[0, 0], traj[0, 1], 'o', color=color, \n",
    "               markersize=4)\n",
    "        if len(traj) > 1:\n",
    "            ax.plot(display_traj[-1, 0], display_traj[-1, 1], 's', \n",
    "                   color=color, markersize=4)\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Échec 1 : Impact du Learning Rate (Divergence)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/temp/echec1_lr_divergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Graphe de convergence\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "for (name, costs), color in zip(costs_fail1.items(), colors):\n",
    "    if not np.isnan(costs[0]):\n",
    "        # Limiter aux 30 premières itérations pour voir la divergence\n",
    "        display_costs = costs[:min(30, len(costs))]\n",
    "        ax.plot(display_costs, color=color, linewidth=2, label=name, marker='o')\n",
    "\n",
    "ax.set_xlabel('Itération', fontsize=12)\n",
    "ax.set_ylabel('Coût f(x, y)', fontsize=12)\n",
    "ax.set_title('Divergence : Coût qui MONTE au lieu de descendre', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')  # Échelle log pour voir les explosions\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/temp/echec1_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Learning Rate trop petit : Stagnation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"CAS D'ÉCHEC 2 : Learning Rate Trop Petit → Stagnation\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Rosenbrock avec learning rate minuscule\n",
    "x0 = np.array([-1.0, 1.0])\n",
    "learning_rates_slow = [0.001, 0.0001, 0.00001]\n",
    "\n",
    "trajectories_fail2 = {}\n",
    "costs_fail2 = {}\n",
    "\n",
    "for lr in learning_rates_slow:\n",
    "    traj, costs = gradient_descent(\n",
    "        rosenbrock, grad_rosenbrock, x0,\n",
    "        learning_rate=lr, max_iter=1000\n",
    "    )\n",
    "    trajectories_fail2[f'α={lr}'] = traj\n",
    "    costs_fail2[f'α={lr}'] = costs\n",
    "    \n",
    "    # Diagnostic\n",
    "    distance_to_optimum = np.linalg.norm(traj[-1] - np.array([1.0, 1.0]))\n",
    "    progress = costs[0] - costs[-1]\n",
    "    \n",
    "    print(f\"α={lr} : {len(traj)} itérations\")\n",
    "    print(f\"  Coût initial : {costs[0]:.4f}\")\n",
    "    print(f\"  Coût final   : {costs[-1]:.4f}\")\n",
    "    print(f\"  Progrès      : {progress:.4f}\")\n",
    "    print(f\"  Distance à (1,1) : {distance_to_optimum:.4f}\")\n",
    "    \n",
    "    if distance_to_optimum > 0.1:\n",
    "        print(f\"  ❌ ÉCHEC : N'a pas atteint le minimum\")\n",
    "    elif len(traj) >= 1000:\n",
    "        print(f\"  ⚠️  LENT : A atteint max_iter\")\n",
    "    else:\n",
    "        print(f\"  ✅ OK mais lent\")\n",
    "    print()\n",
    "\n",
    "# Graphe de comparaison\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "x = np.linspace(-2, 2, 200)\n",
    "y = np.linspace(-1, 3, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = rosenbrock(np.array([X[i, j], Y[i, j]]))\n",
    "\n",
    "ax.contour(X, Y, Z, levels=50, cmap='viridis', linewidths=0.8)\n",
    "ax.contourf(X, Y, Z, levels=50, cmap='viridis', alpha=0.3)\n",
    "\n",
    "colors = ['#E07A5F', '#E9A319', 'yellow']\n",
    "for (name, traj), color in zip(trajectories_fail2.items(), colors):\n",
    "    # Afficher seulement chaque 10e point pour lisibilité\n",
    "    display_indices = range(0, len(traj), 10)\n",
    "    display_traj = traj[display_indices]\n",
    "    \n",
    "    ax.plot(display_traj[:, 0], display_traj[:, 1], \n",
    "           color=color, linewidth=2, alpha=0.7,\n",
    "           label=f'{name} ({len(traj)} itér.)')\n",
    "    \n",
    "    # Début et fin\n",
    "    ax.plot(traj[0, 0], traj[0, 1], 'o', color='#6A994E', markersize=4)\n",
    "    ax.plot(traj[-1, 0], traj[-1, 1], 'o', color='#E07A5F', markersize=4)\n",
    "\n",
    "# Marquer le vrai minimum\n",
    "ax.plot(1.0, 1.0, 'o', color='#457B9D', markersize=6, label='Minimum global (1,1)')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Échec 2 : Learning Rate Trop Petit (Stagnation)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/temp/echec2_lr_stagnation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Graphe de convergence\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "for (name, costs), color in zip(costs_fail2.items(), colors):\n",
    "    ax.plot(costs, color=color, linewidth=2, label=name)\n",
    "\n",
    "ax.set_xlabel('Itération', fontsize=12)\n",
    "ax.set_ylabel('Coût f(x, y)', fontsize=12)\n",
    "ax.set_title('Stagnation : Descente Extrêmement Lente', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/temp/echec2_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Minimum Local : Piège d'Ackley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"CAS D'ÉCHEC 3 : Minimum Local (Piège d'Ackley)\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Tester Ackley avec plusieurs points de départ\n",
    "starting_points = [\n",
    "    (\"Proche (1,1)\", np.array([1.0, 1.0])),\n",
    "    (\"Moyen (3,3)\", np.array([3.0, 3.0])),\n",
    "    (\"Loin (5,5)\", np.array([5.0, 5.0]))\n",
    "]\n",
    "\n",
    "trajectories_fail3 = {}\n",
    "costs_fail3 = {}\n",
    "\n",
    "for name, x0 in starting_points:\n",
    "    traj, costs = gradient_descent(\n",
    "        ackley, grad_ackley, x0,\n",
    "        learning_rate=0.01, max_iter=500\n",
    "    )\n",
    "    trajectories_fail3[name] = traj\n",
    "    costs_fail3[name] = costs\n",
    "    \n",
    "    # Diagnostic\n",
    "    final_point = traj[-1]\n",
    "    final_cost = costs[-1]\n",
    "    distance_to_global = np.linalg.norm(final_point)\n",
    "    \n",
    "    print(f\"{name} : départ {x0}\")\n",
    "    print(f\"  Arrivée : ({final_point[0]:.4f}, {final_point[1]:.4f})\")\n",
    "    print(f\"  Coût final : {final_cost:.6f}\")\n",
    "    print(f\"  Distance à (0,0) : {distance_to_global:.4f}\")\n",
    "    \n",
    "    if final_cost < 0.1:\n",
    "        print(f\"  ✅ SUCCÈS : A trouvé le minimum global\")\n",
    "    else:\n",
    "        print(f\"  ❌ ÉCHEC : Bloqué dans un minimum local\")\n",
    "    print()\n",
    "\n",
    "# Graphe de comparaison\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "x = np.linspace(-6, 6, 300)\n",
    "y = np.linspace(-6, 6, 300)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = ackley(np.array([X[i, j], Y[i, j]]))\n",
    "\n",
    "ax.contour(X, Y, Z, levels=50, cmap='viridis', linewidths=0.8)\n",
    "ax.contourf(X, Y, Z, levels=50, cmap='viridis', alpha=0.3)\n",
    "\n",
    "colors = ['#6A994E', '#E9A319', '#E07A5F']\n",
    "for (name, traj), color in zip(trajectories_fail3.items(), colors):\n",
    "    ax.plot(traj[:, 0], traj[:, 1], \n",
    "           color=color, linewidth=2, alpha=0.8,\n",
    "           label=f'{name} ({len(traj)} itér.)')\n",
    "    \n",
    "    # Début et fin\n",
    "    ax.plot(traj[0, 0], traj[0, 1], 'o', color=color, markersize=4)\n",
    "    ax.plot(traj[-1, 0], traj[-1, 1], 's', color=color, markersize=4)\n",
    "\n",
    "# Marquer le minimum global\n",
    "ax.plot(0.0, 0.0, 'o', color='#457B9D', markersize=6, label='Minimum GLOBAL (0,0)')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Échec 3 : Piège des Minima Locaux (Ackley)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/temp/echec3_minima_locaux.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Graphe de convergence\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "for (name, costs), color in zip(costs_fail3.items(), colors):\n",
    "    ax.plot(costs, color=color, linewidth=2, label=name, marker='o', markersize=3)\n",
    "\n",
    "# Ligne pour montrer le seuil de \"succès\"\n",
    "ax.axhline(y=0.1, color='#457B9D', linestyle='--', linewidth=2, \n",
    "          label='Seuil succès (f<0.1)')\n",
    "\n",
    "ax.set_xlabel('Itération', fontsize=12)\n",
    "ax.set_ylabel('Coût f(x, y)', fontsize=12)\n",
    "ax.set_title('Convergence : Global vs Locaux', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/temp/echec3_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Momentum trop élevé : Oscillations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"CAS D'ÉCHEC 4 : Momentum trop élevé → Oscillations\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Tester différents momentum sur quadratique\n",
    "x0 = np.array([5.0, 5.0])\n",
    "momentum_values = [0.5, 0.9, 0.95, 0.99]\n",
    "\n",
    "trajectories_fail4 = {}\n",
    "costs_fail4 = {}\n",
    "\n",
    "for beta in momentum_values:\n",
    "    traj, costs = gradient_descent_momentum(\n",
    "        quadratique, grad_quadratique, x0,\n",
    "        learning_rate=0.1, momentum=beta, max_iter=200\n",
    "    )\n",
    "    trajectories_fail4[f'β={beta}'] = traj\n",
    "    costs_fail4[f'β={beta}'] = costs\n",
    "    \n",
    "    # Diagnostic\n",
    "    final_cost = costs[-1]\n",
    "    # Compter les oscillations (coût qui monte puis descend)\n",
    "    oscillations = sum(1 for i in range(1, len(costs)-1) \n",
    "                      if costs[i] > costs[i-1] and costs[i] > costs[i+1])\n",
    "    \n",
    "    print(f\"β={beta} : {len(traj)} itérations, {oscillations} oscillations\")\n",
    "    print(f\"  Coût final : {final_cost:.2e}\")\n",
    "    \n",
    "    if final_cost < 1e-6:\n",
    "        if oscillations > 10:\n",
    "            print(f\"  ⚠️  Converge mais avec beaucoup d'oscillations\")\n",
    "        else:\n",
    "            print(f\"  ✅ Converge normalement\")\n",
    "    else:\n",
    "        print(f\"  ❌ N'a pas convergé correctement\")\n",
    "    print()\n",
    "\n",
    "# Graphe de comparaison\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "x = np.linspace(-6, 6, 200)\n",
    "y = np.linspace(-6, 6, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = quadratique(np.array([X[i, j], Y[i, j]]))\n",
    "\n",
    "ax.contour(X, Y, Z, levels=30, cmap='viridis', linewidths=0.8)\n",
    "ax.contourf(X, Y, Z, levels=30, cmap='viridis', alpha=0.3)\n",
    "\n",
    "colors = ['#6A994E', '#457B9D', '#E9A319', '#E07A5F']\n",
    "for (name, traj), color in zip(trajectories_fail4.items(), colors):\n",
    "    ax.plot(traj[:, 0], traj[:, 1], \n",
    "           color=color, linewidth=1.5, alpha=0.7,\n",
    "           label=f'{name} ({len(traj)} itér.)')\n",
    "    \n",
    "    # Début\n",
    "    ax.plot(traj[0, 0], traj[0, 1], 'o', color='#6A994E', markersize=4)\n",
    "    \n",
    "    # Dernier point\n",
    "    ax.plot(traj[-1, 0], traj[-1, 1], 'o', color=color, markersize=4)\n",
    "\n",
    "ax.plot(0, 0, 'o', color='#457B9D', markersize=6, label='Optimum (0,0)')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Échec 4 : Momentum Excessif (Oscillations)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/temp/echec4_momentum_oscillations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Graphe de convergence\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "for (name, costs), color in zip(costs_fail4.items(), colors):\n",
    "    ax.plot(costs, color=color, linewidth=2, label=name, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Itération', fontsize=12)\n",
    "ax.set_ylabel('Coût f(x, y)', fontsize=12)\n",
    "ax.set_title('Oscillations dues au Momentum Excessif', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/temp/echec4_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Ravine étroite : Zigzags Extrêmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\"*70)\n",
    "print(\"CAS D'ÉCHEC 5 : Ravine Étroite → Zigzags Extrêmes\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "# Fonction quadratique très mal conditionnée\n",
    "def quadratique_extreme(x):\n",
    "    \"\"\"f(x,y) = x² + 100y² - ravine très étroite selon x\"\"\"\n",
    "    return x[0]**2 + 100*x[1]**2\n",
    "\n",
    "def grad_quadratique_extreme(x):\n",
    "    return np.array([2*x[0], 200*x[1]])\n",
    "\n",
    "x0 = np.array([10.0, 10.0])\n",
    "\n",
    "# Comparer Simple vs Momentum\n",
    "traj_simple_zig, costs_simple_zig = gradient_descent(\n",
    "    quadratique_extreme, grad_quadratique_extreme, x0,\n",
    "    learning_rate=0.01, max_iter=300\n",
    ")\n",
    "\n",
    "traj_momentum_zig, costs_momentum_zig = gradient_descent_momentum(\n",
    "    quadratique_extreme, grad_quadratique_extreme, x0,\n",
    "    learning_rate=0.01, momentum=0.9, max_iter=300\n",
    ")\n",
    "\n",
    "print(f\"Simple : {len(traj_simple_zig)} itérations\")\n",
    "print(f\"  Coût final : {costs_simple_zig[-1]:.2e}\")\n",
    "\n",
    "print(f\"\\nMomentum : {len(traj_momentum_zig)} itérations\")\n",
    "print(f\"  Coût final : {costs_momentum_zig[-1]:.2e}\")\n",
    "\n",
    "# Graphe de comparaison\n",
    "fig, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "x = np.linspace(-12, 12, 200)\n",
    "y = np.linspace(-12, 12, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = quadratique_extreme(np.array([X[i, j], Y[i, j]]))\n",
    "\n",
    "ax.contour(X, Y, Z, levels=30, cmap='viridis', linewidths=0.8)\n",
    "ax.contourf(X, Y, Z, levels=30, cmap='viridis', alpha=0.3)\n",
    "\n",
    "# Simple en rouge (zigzags)\n",
    "ax.plot(traj_simple_zig[:, 0], traj_simple_zig[:, 1], \n",
    "       '-', color='#E07A5F', linewidth=1.5, alpha=0.7, label=f'Simple ({len(traj_simple_zig)} itér.)')\n",
    "\n",
    "# Momentum en bleu (plus lisse)\n",
    "ax.plot(traj_momentum_zig[:, 0], traj_momentum_zig[:, 1], \n",
    "       '-', color='#457B9D', linewidth=1.5, alpha=0.7, label=f'Momentum ({len(traj_momentum_zig)} itér.)')\n",
    "\n",
    "# Points de départ et arrivée\n",
    "ax.plot(10, 10, 'o', color='#6A994E', markersize=4, label='Départ')\n",
    "ax.plot(0, 0, 'o', color='#457B9D', markersize=6, label='Optimum (0,0)')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('y', fontsize=12)\n",
    "ax.set_title('Échec 5 : Zigzags dans Ravine Étroite (ratio 1:100)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/temp/echec5_zigzags_ravine.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Graphe de convergence\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "ax.plot(costs_simple_zig, '-', color='#E07A5F', linewidth=2, label='Simple', alpha=0.7)\n",
    "ax.plot(costs_momentum_zig, '-', color='#457B9D', linewidth=2, label='Momentum', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Itération', fontsize=12)\n",
    "ax.set_ylabel('Coût f(x, y)', fontsize=12)\n",
    "ax.set_title('Convergence : Simple vs Momentum dans Ravine', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/temp/echec5_convergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tableau récapitulatif des échecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TABLEAU RÉCAPITULATIF DES CAS D'ÉCHEC\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_echecs = {\n",
    "    'Cas d\\'Échec': [\n",
    "        '1. LR trop grand',\n",
    "        '2. LR trop petit',\n",
    "        '3. Minimum local',\n",
    "        '4. Momentum excessif',\n",
    "        '5. Ravine étroite'\n",
    "    ],\n",
    "    'Symptôme': [\n",
    "        'Divergence, explosion',\n",
    "        'Stagnation, progression lente',\n",
    "        'Converge mais pas au global',\n",
    "        'Oscillations persistantes',\n",
    "        'Zigzags inefficaces'\n",
    "    ],\n",
    "    'Cause': [\n",
    "        'α > seuil stabilité',\n",
    "        'α << gradient',\n",
    "        'Point initial éloigné',\n",
    "        'β proche de 1.0',\n",
    "        'Fonction mal conditionnée'\n",
    "    ],\n",
    "    'Solution': [\n",
    "        'Réduire α ou use Adam',\n",
    "        'Augmenter α prudemment',\n",
    "        'Momentum/Adam ou restart',\n",
    "        'Réduire β (0.9 typique)',\n",
    "        'Momentum ou Adam'\n",
    "    ],\n",
    "    'Graphe': [\n",
    "        'echec1_lr_divergence.png',\n",
    "        'echec2_lr_stagnation.png',\n",
    "        'echec3_minima_locaux.png',\n",
    "        'echec4_momentum_oscillations.png',\n",
    "        'echec5_zigzags_ravine.png'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_echecs = pd.DataFrame(data_echecs)\n",
    "print(df_echecs.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOTAL : 10 graphes d'échecs générés (2 par cas)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Notes pour le rapport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OBSERVATIONS POUR LE RAPPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\"\"\n",
    "1. FONCTION QUADRATIQUE\n",
    "   - Tous les algorithmes convergent\n",
    "   - Simple : zigzags visibles dus à la différence de courbure\n",
    "   - Momentum/Nesterov : trajectoires plus lisses\n",
    "   - Adam : convergence la plus rapide\n",
    "\n",
    "2. ROSENBROCK\n",
    "   - Fonction très difficile (vallée étroite)\n",
    "   - Simple/Momentum/Nesterov : très lents\n",
    "   - Adam : beaucoup plus efficace grâce à l'adaptation du learning rate\n",
    "\n",
    "3. BOOTH\n",
    "   - Convergence rapide pour tous les algorithmes\n",
    "   - Paysage relativement simple\n",
    "\n",
    "4. BEALE\n",
    "   - Fonction avec de forts gradients près de l'origine\n",
    "   - Nécessite un learning rate plus petit\n",
    "   - Adam s'adapte automatiquement\n",
    "\n",
    "5. HIMMELBLAU\n",
    "   - 4 minima globaux équivalents\n",
    "   - Le choix du point de départ détermine quel minimum est atteint\n",
    "   - Tous les algorithmes convergent vers un des minima\n",
    "\n",
    "CONCLUSION :\n",
    "- Simple : fonctionne mais lent et zigzague\n",
    "- Momentum : améliore Simple mais sensible aux paramètres\n",
    "- Nesterov : légèrement meilleur que Momentum\n",
    "- Adam : le plus robuste, s'adapte automatiquement\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
